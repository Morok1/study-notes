# -*- org-src-fontify-natively t -*-
#+TITLE: Конспект по ПП/Распределенные системы от Елизарова (3 курс)

* Параллельное программирование
** Какие-то материалы:
   * https://www.slideshare.net/secret/6dv9wtl1pDOTgu
   * https://www.slideshare.net/secret/h3wT59xRPciKM6
   * https://www.slideshare.net/secret/yDV9vtRs61gEdg
   * https://www.slideshare.net/secret/E1asIyWYNfvkNv
   * https://www.slideshare.net/secret/LnSxMJ7JZ25JFe
   * Консенсус:
     https://www.slideshare.net/secret/LnSxMJ7JZ25JFe
   * Реализация ссылочных структуры:
     https://www.slideshare.net/secret/zrtgOfnRQY6K0Z
   * Массивы, CASN, R/W Locks, STM:
     https://www.slideshare.net/secret/8ZkekdbzRSIjsP
   * Рекомендуемые учебники:
     * The Art of Multiprocessor Programming, Maurice Herlihy, Nir
       Shavit, http://www.amazon.com/The-Multiprocessor-Programming-..
       Это основной учебник для осеннего семестра. Его надо знать и
       любить.
     * Concurrent and Distributed Computing in Java, Vijay K. Garg,
       http://www.amazon.com/Concurrent-Distributed-Computin..  Обзор
       для всех частей курса (пригодится так же и для второго
       семестра).
** Вступление, модели, их свойства, согласованность
   #+DATE: 07.09.2015
   *SMP* (/symmetric multiprocessing/) -- два или больше ядра, на каждом по
   потоку.

   *SMT* (/simultaneous multithreading/) -- два или больше потоков
   исполняются одним ядром.

   *NUMA* (/not uniform memory access/) -- все знают, что это еще с 1
   курса.

   Операционные системы (классификация по параллельности):
   1. Однозадачные
   2. Пакетные задания (/batch processing/)
   3. Многозадачные
      1. Кооперативная (/cooperative/) многозадачность.
         Переход от одной задачи к другой происходит явно, когда
         программа готова передать процессорное время другой.
      2. Вытесняющая (/preemptive/) многозадачность.
         Текущий подвид -- ОС сама расставляет инструкции как ей этого
         хочется.

   Поток и процесс (на практике вперемешку):
   * *Процесс* -- владеет памятью и ресурсами.
   * *Поток* -- контекст исполнения внутри процесса.

   Модели программирования:
   1. Однопоточное/однозадачное.
   2. Многозадачное:
      1. Модель с общей памятью:
         Единственное, что объединяет потоки -- общие объекты.
         Общий объект -- переменная (~read/write~, тип). Иногда зовутся
         регистрами, так как на практике значения лежат как раз в
         регистрах ЦП.
      2. Модель с передачей сообщений.

   Модели исполнения:
   1. Модель чередований
      Строим дерево, в котором описываем все возможные перестановки
      операций над общими объектами. Такая модель не описывает
      появление пары ~(0, 0)~ в следующем примере:

      #+NAME: java no-volatile
      #+BEGIN_SRC text
        thread P {
               x = 1
               r1 = y
        }
        thread Q {
               y = 1
               r2 = x
        }
      #+END_SRC

      Но оно может появиться вследствие того факта, что компилятор
      может переставить инструкции или же просто запись обоих значений
      будет отложена процессором.

      Модель чередования не описывает конкретно физических реалий этого
      мира -- ведь свет за один такт процессора проходит 10см, элементы
      просто физически не могут синхронизоваться.

      Физическая -- это световой конус.
   2. Happens before
      Исполнение системы -- пара $⟨H, →⟩$.
      * $H$ -- множество операций (чтение и запись ячеек памяти)
      * $e → f$ значит, что $e$ произошло раньше $f$ -- частичный строгий
        порядок на \(H\).
      * $a ∥ b$, если $¬(a → b ∨ b → a)$.
   3. Модель глобального времени
      Каждая операция -- интервал $[t_{inv}, t_{resp}]$, и если выражать
      через happens-before, то $a → b ≡ t_{resp}(a) < t_{inv}(b)$.

   Конфликты:
   * *Конфликт* (/data race/) -- ситуация, когда происходит две
     параллельных операции, и одна из них -- запись.
   * Корректно синхронизированная программа -- программа без
     конфликтов.

   Исполнения:
   1. *Последовательное* -- все операции линейно упорядочены отношением
      happens-before (→). В модели глобального времени ничего не
      наслаивается вообще.
   2. *Правильное* -- сужение исполнения на один поток (только
      операции, приналежащие конкретному потоку) последовательно.
      Неправильные исполнения -- вообще какая-то чепуха (параллельность
      в одном потоке). Или "~a = (b++) + (c++)~" -- undefined behavior
      в плюсах.
   3. *Допустимое последовательное* исполнение -- выполнены
      последовательные спецификации всех объектов. Посл. спецификация
      объекта -- последовательность сужения исполнения на конкретный
      объект.

   Условия согласованности:
   1. Последовательная согласованность
      Исполнение посл. согласованно, если можно сопоставить ему
      допустимое последовательное исполнение, причем программный
      порядок (≡ порядок операций на каждом потоке) сохраняется.

      Кстати последовательная согласованность на каждом объекте не
      влечет за собой последовательную согласованность исполнения.
   2. Линеаризуемость
      Исполнение линеаризуемо, если можно сопоставить ему
      допустимое последовательное исполнение, которое сохраняет порядок
      happens-before.

      Линеаризуемость локальна, линеаризуемость на каждом объекте
      влечет линеаризуемость системы.
      Операции над линеаризуемыми объектами называют атомарными.
      Исполнение системы, выполняющее операции над линеаризуемыми
      объектами, можно анализировать в модели чередования.
      Свойство thread-safe объекта есть ровно линеаризуемость.

      Если в примере java-novolatile сделать ~x,y volatile~, то пара
      $(0,0)$ не будет появляться, то есть исполнение действительно
      соответствует модели чередования.

      Реализуется ~volatile~ в java локами памяти (/memory lock/). Тут
      мы немного теряем производительность, но не страшно.
** Блокировки, взаимное исключение, deadlock'и
   #+DATE: 14.09.2015
   *Линеаризуемость* -- суперское свойство.  Даже если в каждом потоке
   все операции атомарны, мы не можем утверждать, что объект
   линеаризуем (ну представьте себе класс очереди с методами
   ~push/pop~).  Мы вот хотим линеаризуемую очередь.

   Самый простой метод сделать так -- это использование блокировок
   (/locks/, /mutex/ (/mutual exclusion/)).

   Идея: заведем в объекте ~member Mutex m~, будем его лочить и
   разлочивать. Этот объект будет гарантировать, что объект будет
   застрявать на ~.lock~, если мьютекс уже залочен и т.д.  Код с
   mutex'ами будет thread-safe, если операции будут защищены одним и
   тем же мьютексом.

   * *Взаимное исключения* -- свойство исполнения, при котором
     критические секции не могут выполняться параллельно -- это
     требование корректности взаимного исключения.

   При этом взаимное исключение имеет ряд условных условий прогресса:
     1. *Свобода взаимной блокировки* (/deadlock-freedom/) -- свойство
        взаимного исключения, при котором если несколько потоков
        пытаются войти в критическую секцию, то хотя бы один сделает
        это за конечное время (при условии что критические секции сами
        по себе конечны).
     2. *Отсутствие голодания* (/starvation-freedom/) -- если какой-то
        поток пытается войти в критическую секцию, он сделает это за
        конечное время (опять-таки, при условии, что крит. секции
        исп. за конечное время).
     3. *Линейное ожидание* -- каждый поток совершает $O(n)$ действий
        перед тем, как войти в критическую секцию (условие аналогично)
     4. *First Come First Served* (/FSFS/) -- свойство сильнее
        линейного ожидания, потоки обслуживаются в порядке утыкания в
        критическую секцию (условие аналогично).

   Как написать mutex, собственно?
   1. *Aлгоритм Петерсона* -- гарантирует взаимное исключение,
      отсутствие взаимной блокировки и отсутствие голодания.

      Преимущество -- самый простой.
      #+BEGIN_SRC text
        threadlocal int id // 0 or 1
        shared boolean want[2]
        shared int     victim

        def lock:
            want[id] = true
            victim = id
            while (want[1-id] and victim == id) {}

        def unlock:
            want[id] = false
      #+END_SRC
   3. *Aлгоритм Петерсона для N потоков* (/filter algorithm/).

      Все то же самое, но может делать $O(N²)$ ожидания.
      #+BEGIN_SRC text
        threadlocal int id
        shared int level[N]
        shared int victim[N]

        def lock:
            for j = 1..N-1:
                level[id] = j
                victim[j] = id
                while exist k: k != id and
                               level[k] >= j and
                               victim[j] == id:
                               {}

        def unlock:
            level[id] = 0
      #+END_SRC
   4. Алгоритм *Лампорта* (булочника -- 1 вариант).  Обладает свойством
      FCFS. Это вариант с бесконечными метками ~label~. Можно сделать с
      конечными.

      Первые две строки ~lock~ называются ~doorway~.
      #+BEGIN_SRC text
        threadlocal int id
        shared boolean want[N]  // init false
        shared int     label[N] // init 0

        def lock:
            want[id] = true
            label[id] = max(label) + 1
            while exists k: k != id and
                            want[k] and
                            (label[k], k) < (label[id], id)
                            {}
      #+END_SRC

   Блокировки бывают грубыми и тонкими.
   * *Грубая* -- блокировать всю операцию целиком.
   * *Тонкая* -- блокировать операции над общими объектами внутри, а не
     вызов, но тогда необходима двухфазовая блокировка.

   Есть проблема deadlock'а. Допустим, что есть два mutex'а, мы лочимся
   в одном треде сначала по ~m1~, потом по ~m2~, в другом треде
   наоборот. Можем задедлочиться тут короче.

   *Закон Амдала* для параллельной работы: \[speedup = \frac{1}{(S +
   \frac{1-S}{N})}\] Это максимальное ускорение при запуске кода в $N$
   потоков, если доля кода $S$ выполнена последовательно.
** Алгоритмы/объекты без блокировок, свободы (lock/wait/obstr)
   #+DATE: 21.09.2015
   Алгоритмы без блокировок.

   Безусловные условия прогресса:
   1. *Obstruction-free* (/отсутствие помех/) -- свойство алгоритма, в
      котором если остановить всe потоки кроме одного (любого) в любом
      месте, один должен завершиться за конечное время. Так должно
      работать для каждого объекта.  Очевидно, что объект с блокировкой
      не имеет такого свойства.
   2. *Lock-freedom* -- если много потоков пытаются сделать операцию,
      то хотя бы один поток должен ее исполнить за конечное
      время. Плохо то, что это условие не исключает голодания.
   3. *Wait-freedom* (самое сильное условие) -- если какой-то поток
      пытается выполнить операцию, то он это сделает (вне зависимости
      от действия/бездействия других потоков).

   Объекты без блокировки. ОБъекты бывают с lock-freedom, но этот
   термин перегружен.
   * Регистры без блокировки
     * Свойства физических регистров:
       1. Неатомарны.
       2. Работают без ожидания.
       3. Предполагают только одного читателя и одного писателя.
       4. Попытка записать и прочитать одновременно -- UB.
       5. Они безопасные (/safe/) -- в смысле, после записи, будет
          прочитано последнее записанное значение.
     * Классификация регистров
       1. По условиям согласованности:
          1. *Безопасные* (/safe/) -- гарантирует получение последнего
             записанного значения, если операция чтения не параллельна
             операции записи.
          2. *Регулярные* (/regular/) -- при чтении выдает последнее
             записанное, или то, что уже пишется.
          3. *Атомарные* (/atomic/) -- линеаризуемое (как регулярный,
             только если уже прочитал новое значение, то старое нельзя
             прочитать).

             Как проверить регистр на атомарность в схеме глобального
             времени -- поставить в каждой полоске точку,
             соответствующую этому конкретному действию. Порядок по
             точкам должен быть атомаррным.
       2. По количеству потоков (~SR~, ~MR~, ~SW~, ~MW~ :
          ~single/multi~ ~reader/writer~).

     * Будем строить более сложные регистры из простых требуя
       wait-free условие.
       Пусть у нас есть Safe SRSW Boolean регистр.
       1. Regular SRSW Boolean.

          #+BEGIN_SRC text
            safe shared boolean r
            threadlocal boolean last

            def write(x):
              if (x != last)
                last = x
                r = x

            def read(): return r
          #+END_SRC
       2. Regular SRSW M-Valued.

          Пусть у нас массив ~r~ хранит булевые значения, и число в нем
          хранится последовательностью единиц, терминированной
          нулем. Тогда это реализуется так:

          #+BEGIN_SRC text
            regular shared boolean[M] r

            def write(x): // Справа налево
              r[x] = 0
              for i = x-1 downto 0: r[i] = 1

            def read(): // Слева направо
              for i = 0 to M-1: if r[i] == 0: return i
          #+END_SRC
       3. Atomic SRSW M-Valued.

          Будем хранить пару -- значение и версию. Версию можно разумно
          ограничить. Есть алгоритм без жульничества с версиями, но он
          на практике плох.

          #+BEGIN_SRC text
            safe shared (int x, int v) r
            threadlocal (int x, int v) lastRead
            threadlocal int lastWriteV

            def write(x):
              lastWriteV++
              r = (x, lastWriteV)

            def read():
              cur = r
              if cur.v > lastRead.v:
                lastRead = cur
              return lastRead.x
          #+END_SRC

          Атомарный регистр: проблемы
          1. *Версии* -- могут хранить пару в регуярном, но версии
             растут неограниченно.
          2. *Блокировки* -- алгоритм Лампорта будет работать на
             регулярных регистрах, но это не дает алгоритм без ожидания.

          * Теорема: не существует алгоритма построения атомарного
            регистра без ожидания, который использует конечное число
            регулярных регистров конечного размера так, чтобы их писал
            только писатель, а читал только читатель
          * Доказательство
            Нужна обратная связь от читателя к писателю.
       4. Atomic MRSW M-Valued.

          Нужно отслеживать версию записанного значения, храня пару
          $(x, v)$ в каждом из $N$ регистров в которые пишет писатель.
          Наивно сделать это нельзя.

          Заведем $N×(N-1)$ регистров для общения между читателями.

          1. Каждый читатель выбирает более позднее значение из
             записанного писателем и из прочитанных значенией других
             читателей
          2. Читатель записывает свое прочитанное значение и версию для
             всех остальных читателей.
       5. Atomic MRMW M-Valued.

          Нужна поддержка $N$ писателей.

          Отслеживаем версию записанного значения:
          1. Каждый читатель выбирает более позднюю версию
          2. Для проставления версий писателями используем doorway
             секцию из алгоритма булочника (Лампорта)
             * Версия состоит из пары номера потока писателя и
               собственно числа
     * Атомарный снимок состояния N регистров.

       Наивная реализация не обеспечивает атомарность.

       Вот этот алгоритм уже lock-free, но достаточно наивный --
       читает, пока массивы не совпадут.
       #+NAME: lock-free implementation of atomic registers snapshot
       #+BEGIN_SRC text
         shared (int x, int v) r[N]

         // wait-free
         def update(i, x):
             r[i] = (x, r[i].v + 1)

         // lock-free
         def scan():
             old = copy()
             loop:
                 cur = copy()
                 if forall i: cur[i].v == old[i].v
                    return cur.x  // we can get starvation here,
                                  // if update is executed too frequent
                 old = cur
       #+END_SRC

       Вот wait-free реализация с костылями.
       #+NAME: wait-free implementation
       #+BEGIN_SRC text
         shared (int x, int v, int[N] s) r[N]

         def update():
             s = scan()
             r[i] = (x, r[i].v + 1, s)

         shared (int x, int v, int[N] s) r[N]

         // wait-free, O(N^2)
         def scan():
             old = copy()
             boolean updated[N]
             loop:
                 cur = copy()
                 for i = 0..N-1:
                     if cur[i].v != old[i].v:
                        if updated[i]: return cur.s
                        else:
                         update[i] = true
                         old = cur
                         continue loop
                 return cur.x
       #+END_SRC
       * Лемма: Если значение поменялось второй раз, значит копия
         снимка $s$ была получена вложенной операцией ~scan~.
** Консенсус
   #+DATE: 05.10.2015

   *Консенсус* -- это объект, который помогает $n$ потокам придти к общему
   мнению.
   #+BEGIN_SRC text
     class Consensus:
           def decide(val):
           ...
           return decision
   #+END_SRC

   Каждый поток использует объект ~Consensus~ один раз.
   Требуются две вещи:
   * *Согласованность* (/consistency/): все потоки должны вернуть одно
     и то же значение из метода decide.
   * *Обоснованность* (/validity/): возвращенное значение было входным
     значением какого-то из потоков.

   #+NAME: Консенсус с блокировкой
   #+BEGIN_SRC text
     shared int decision
     Mutex mutex

     def decide(val):
         mutex.lock()
         if (decision == NA):
            decision = val
         mutex.unlock()
         return decision
   #+END_SRC
   Но мы хотим без ожидания.

   Все не так просто.
   Консенсусное число:
   1. Если с помощью класса атомарных объектов $С$ и атомарных регистров
      можно реализовать консенсусный протокол без ожидания для $N$
      потоков (и не больше), то говорят что у класса $С$ консенсусное
      число равно $N$.
   2. Теорема:
      Атомарные регистры имеют консенсусное число 1.
      * То есть с помощью атомарных регистров даже 2 потока не могут
        придти к консенсусу без ожидания (докажем от противного) для 2х
        возможных значений при $T = \{0, 1\}$
      * С ожиданием задача решается очевидно (с помощью любого
        алгоритма взаимного исключения).
   3. Определения и леммы для любых классов объектов:
      * Определения и концепции
        1. Исходныe объекты атомарны. Любое исполнение можно
           рассматривать как последовательное в каком-то порядке.
        2. Рассматриваем дерево состояния, листья -- конечные состояния
           помеченные 0 или 1 (в зависимости от значения консенсуса).
        3. *x-валентное* состояние системы ($x ∈ \{0,1\}$) -- консенсус
           по всех нижестоящих листьях будет x.
        4. *Бивалентное* состояние -- возможен консенсус как 0 так и 1.
        5. *Критическое* состояние -- такое бивалентное состояние, все
           дети которого одновалентны.
      * Лемма: Существует начальное бивалентное состояние.

        Это нетривиально следует из того факта, что алгоритм без
        ожиданий.

        Возьмем конечное количество шагов, построим дерево.
        $???$
        Доказательство было на доске и не сохранилось.
      * Лемма: Существует критическое состояние

        Тоже следует из wait-free. Если есть бивалентное, будем
        смотреть его детей. Если есть хотя бы один бивалентный ребенок,
        то спускаемся в него, пока бивалентных детей больше нету.
        За счет конечности дерева такое будет существовать, и
        валентность детей будет различна (иначе валентность самого узла
        тоже определена).

      Для атомарных регистров рассмотрим возможные пары операций в
      критическом состоянии:
      * Операции над разными регистрами коммутируют.
      * Два чтения коммутируют.
      * Любая операция + запись -- состояние пишущего потока не зависит
        от порядка операций. Противоречие (в чем???).
   4. Бывают Read-Modify-Write регистры.

      #+NAME: read-modify-write reg
      #+BEGIN_SRC text
        class RMWRegister:
              private shared int reg

              def read():
                  return reg

              def getAndF(args):
                  do atomically:
                     old = reg
                     reg = F(args)(reg)
                     return old
      #+END_SRC
      Функция F может быть ~getAndSet~, ~getAndIncrement~,...

      #+NAME: Consensus for RMW reg, реализация для 2х потоков
      #+BEGIN_SRC text
        threadlocal int id // 0 or 1

        shared RMWRegister rmw
        shared int proposed[2]

        def decide(val):
            proposed[id] = val
            if (rmw.getAndF() == v0)
                return proposed[i]
            else:
                return proposed[1-i]
      #+END_SRC

      * Консенсусное число нетривиального RMW регистра $≥ 2$.

        Нужно чтобы была хотя бы одна подвижная точка функции $F$,
        например $F(v_0) = v_1 ≠ v_0$.

   5. Common2 RMW регистры
      * $F_1$ и $F_2$ коммутируют если $F_1(F_2(x)) = F_2(F_1(x))$.
      * $F_1$ перезаписывает $F_2$ если $F_1(F_2(x)) = F_1(x)$.
      * Класс $С$ RMW регистров принадлежит Common2 если любая пара
        функций либо коммутирует либо одна из функций перезаписывает
        другую.
      * Теорема: нетривиальный класс Common2 RMW регистров имеет
        консенсусное число 2.

        Третий поток не может отличить глобальное состояние при
        изменении порядка выполнения коммутирующих или перезаписывающих
        операций в критическом состоянии.
   6. Универсальные объекты
      Объект с консенсусным числом $∞$ называется универсальным объектом.
      По определению, с его помощью можно реализовать консенсусный
      протокол для любого числа потоков.

      #+NAME: CAS register
      #+BEGIN_SRC text
        class CASRegister:
              private shared int reg

              def CAS(expect, update):
                  do atomically:
                     old = reg
                     if old == expect:
                        reg = update
                        return true
                     return false
      #+END_SRC

      CAS -- самый популярный универсальный объект, процессоры в том
      или ином виде его реализуют.

      * CAS и консенсус
        #+NAME: реализация протокола через CAS+READ
        #+BEGIN_SRC text
          def decide(val):
              if CAS(NA, val):
                  return val
              else:
                  return read()
        #+END_SRC

      * Универсальность консенсуса. Теорема.
        Любой последовательый объект можно реализовать без ожидания для
        N потоков используя консенсусный протокол для N объектов

        * Такое построение -- универсальная конструкция
        * Следствие 1: С помощью любого класса объектов с консенсусным
          числом N можно реализовать любой объект с консенсусным числом
          ≤ N
        * Следствие 2: С помощью универсального объекта можно
          реализовать вообще любой объект
          * Сначала реализуем консенсус для любого числа потоков (по
            определению универсального объекта)
          * Потом через консенсус любой другой объект используя
            универсальную конструкцию.
        * Доказательство теоремы
          1. Универсальная конструкция без блокировки через CAS
             #+BEGIN_SRC text
               shared CASRegister reg

               def concurrentOperationX(args):
                   loop:
                       old = reg.read()
                       upd = old.deepCopy()
                       res = upd.serialOperationX(args)
                   until reg.CAS(old, upd)
                   return res
             #+END_SRC

             * Без блокировки универсальная конструкция проста и
               проктична, если использовать CAS в качестве примитива.
             * Для реализации через консенсус надо чтобы каждый объект
               консенсуса пользовался потоком один раз
             * Для реализации без ожидания нужно чтобы потоки помогали
               друг другу.
          2. Через консенсус.

             ОБъект -- односвязный список стейтов.
             Последний элемент -- текущий стейт.

             #+NAME: Через консенсус без блокировки
             #+BEGIN_SRC text
               class Node:
                     val               // readonly
                     Consensus next    // init fresh obj

               shared Node root        // readonly
               threadlocal Node last   // init rood

               def concurrentOperationX(args):
                   loop:
                       old = last.val
                       upd = old.deepCopy()
                       res = upd.serialOperationX(args)
                       node = new Node(upd)
                       last = last.next.decide(node)
                   until last == node
                   return res
             #+END_SRC

             * Но с ожиданием

          3. Через консенсус без ожидания
             * Храним в узле операцию, которую нужно выполнить, а не
               результат -- каждый поток обновляет и хранит свою
               локальную копию объекта
             * Нумеруем операции последовательными числами, заведя
               переменную ~seq~. После выполнения прописываем номер
               исполненной операции.
             * Каждй поток хранит последнее ему известное значение
               конца списка в элементе массива ~know[id]~.
             * Каждый поток будет заранее записывать операцию, которую
               он планирует выполнить -- в массиве ~announce~.

             #+NAME: Через консенсус без блокировки
             #+BEGIN_SRC text
               class Node:
                     int seq           // init 0
                     args              // readonly
                     Consensus next    // init fresh obj

               shared Node[] announce // init root
               shared Node[] know // init root

               def concunrrentOperationX(args):
                   announce[id] = new Node(args)
                   know[id] = maxSeqFrom(know)
                   while announce[id].seq == 0
                         Node help =
                              announce[know[id].seq % N]
                         Node prev = help if help.seq == 0
                              else announce[id]
                         know[id] = prev.next.decide(node)
                         know[id].seq = prev.seq + 1
                   know[id] = announce[id]
                   return updateMyLastTo(announce[id])

               def updateMyLastTo(node):
                   while last != node:
                         res = my.serialOperationX(last.args)
                         last = last.next
                         return res
             #+END_SRC
   7. Сводная иерархия
      |--------------------------------------------+--------------------|
      | Объект                                     | Консенсусное число |
      |--------------------------------------------+--------------------|
      | Атомарные регистры                         | 1                  |
      | Снимок состояния нескольких регистров      |                    |
      |--------------------------------------------+--------------------|
      | getAndSet, getAndAdd, очередь, стек        | 2                  |
      |--------------------------------------------+--------------------|
      | Атомарная запись m регистров из m(m+1)/2   | m                  |
      |--------------------------------------------+--------------------|
      | compareAndSet, LoadLinked/StoreConditional | ∞                  |
      |--------------------------------------------+--------------------|
** Практические построения на списке, вступление
   Будем смотреть всякие практические построения на списках.
   Будем писать код уже на джаве настоящей.

   *Java* -- первый язык, в котором появилась модель памяти (/memory
   model/). Почему джава? Трюки c++ (~if_arch_~...) не работают в джаве,
   джава очень WORA, и прочее.

   *JMM* определяет:
   1. Межпоточные действия -- чтение и запись,
      синхронизация. Синхронизация -- ~volatile~/~synchronized~/запуск или
      остановка потоков.
   2. Отношение синхронизации (/synchronizes-with/) и отношение
      happens-before.
      Java гарантирует, что если в программе нету гонок, то исполнение
      последовательно согласовано (а значит и линеаризуемо).
   3. Всякие гонки и прочее.

   Выполнение корректно синхронизированной программы будет выглядеть
   последовательно согласовано. Гонки за данными не могут нарушить
   базовые гарантии безопасности платформы (система типов, все кроме
   ~long/double~ атомарны, все поля гарантированно инициализированы
   нулями, дополнительные гарантии для ~final~ полей).

   #+NAME: рабочий вариант 1 решения того же самого кода без volatile
   #+BEGIN_SRC java
     volatile int flag;
     int value;

     void int() {
         value = 2;
         flag = 1;
     }

     int take() {
         while (flag == 0); // кушаем cpu тут
         return value;
     }
   #+END_SRC

   #+NAME: решение 2, cpu не прогорает
   #+BEGIN_SRC java
     int flag, value;

     void synchronized int() {
         value = 2;
         flag = 1;
     }

     int synchronized take() {
         while (flag == 0); // кушаем cpu тут
         return value;
     }
   #+END_SRC

   Таким образом, мы реализовали thread-safe объект.

** Типы синхронизации на примере списка (LinkedSet)
   * *Многопоточные объект* -- это объект, который можно использовать
     из нескольких потоков без дополнительной внешней синхронизации,
     при этом:
     1. Специфицируется через последовательное поведение.
     2. По умолчанию требуется линеаризуемость операций (редко -- более
        слабые условия).
     3. Редко удается реализовать все операции wait-free. Чаще всего
        делается с блокировками или без них (что на самом деле
        lock-free).

   Типы синхронизации:
   1. Грубая синронизация (~Coarse-grained~).
   2. Тонкая (~fine-grained~).
   3. Оптимистичная (~optimistic~).
   4. Ленивая (~lazy~).
   5. Неблокирующая (~non-blocking~).

   Будем строить многопоточные связанные списки. Массивами пользоваться
   намного эффективней, но они сложнее пишутся.

   #+NAME: Что пытаемся синхронизировать
   #+BEGIN_SRC java
     // инвариант node.key < node.next.key
     class Node {
         final int key;
         final T item;
         Node next;
     }
   #+END_SRC
   Пустой список будет состоять из 2х граничных элементов:
   #+BEGIN_SRC java
     Node head = Node(Integer.MIN_VALUE, null);
     head.next = Node(Integer.MAX_VALUE, null);
   #+END_SRC
*** Грубая синхронизация
    Обеспечиваем синхронизацию через
    ~java.util.concurrent.locks.ReentrantLock lock~.
    Такой подход дает немножко больше функционала чем секции
    ~synchronized~.

    #+NAME: грубая синхронизация списка
    #+BEGIN_SRC java
      class LinkedSet {
          final Node head;
          final Lock lock; // mutex

          boolean contains(int key) {
              lock.lock();
              try {
                  Node curr = head;
                  while (curr.key < key) {
                      curr = curr.next;
                  }
                  return key == curr.key;
              } finally { lock.unlock() }
          }

          boolean add(int key, T item) {
              lock.lock();
              try {
                  Node pred = head, curr = pred.next;
                  while (...) {}
                      /// stuff
              } finally { lock.unlock(); }
          }
          boolean remove (int key, T item) {
              lock.lock();
              try {
                  // stuff
              } finally { lock.unlock; }
          }
      }
    #+END_SRC
*** Тонкая синхронизация
    Обеспечиваем синхроизацию взаимным исключением на каждом
    объекте. При любых операциях одновременно удерживаем блокировку
    текущего и предыдущего элемента, чтобы не потерять инвариант
    ~pred.next == curr~.

    #+NAME: Тонкая синхронизация
    #+BEGIN_SRC java
      class Node {
          final int key;
          final T item;
          final Lock lock;
          Node next;

          void lock() { lock.lock(); }
          void unlock() { lock.unlock(); }
      }

      class LinkedSet {
          boolean contains() {
              Node pred = head; pred.lock();
              Node curr = pred.next; curr.lock();
              try {
                  while (curr.key < key) {
                      // отпускаем блокировку у предыдущего объекта
                      // берем у следующего.
                      pred.unlock();
                      pred = curr;
                      curr = curr.next;
                      curr.lock();
                  }
                  return key == curr.key;
              } finally { curr.unlock(); pred.unlock(); }
          }

          boolean add(int key, T item) {
              Node pred = head; pred.lock();
              Node curr = pred.next; curr.lock();
              try {
                  // addition
                  while (curr.key < key) {
                      pred.unlock(); pred = curr;
                      curr = curr.next; curr.lock();
                  }
                  if (key == curr.key) return false; else {
                      Node node = new Node(key, item);
                      node.next = curr; pred.next = node;
                      return true;
                  }
              } finally { curr.unlock; pred.unlock; }
          }

          boolean remove(int key, T item) {
              Node pred = head; pred.lock();
              Node curr = pred.next; curr.lock();
              try {
                  // removal
              } finally { curr.unlock; pred.unlock; }

          }
      }
    #+END_SRC
*** Оптимистичная синхронизация
    Алгоритм построения:
    1. Ищем элемент без синхронизации, но перепроверяем с
       синхронизацией.
       1. Если перепроверка сломалась, то начинаем операцию заново
       2. Поиск не зациклится, ибо ключи упорядочены, никогда не
          меняются внутри Node, значения next не могут возникнуть
          ниоткуда даже при чтении без синхронизации
    2. Имеет смысл только если обход дешев и быстр, а обход с
       синхронизацией -- наоборот.
    3. Потоки всегда синхронизируются между собой ("synchronizes with")
       через критические секции, поэтому никаких дополнительных
       механизмов не нужно.
    #+NAME:Оптимистичная синхронизация
    #+BEGIN_SRC java
      class LinkedSet {
          // проверяет, что pred является предыдущим для curr
          // идет от начала списка до pred оптимистично, там сравнивает
          boolean validate(Node pred, Node curr) {
              Node node = head;
              while (node.key <= pred.key) {
                  if (node == pred) {
                      return pred.next == curr;
                  }
                  node = node.next;
                  if (node == null) return false;
              }
          }

          boolean contains(int key) {
          retry: while (true) {
                  Node pred = head, curr = pred.next;
                  while (curr.key < key) {
                      pred = curr; curr = curr.next;
                      if (curr == null) continue retry;
                  }
                  pred.lock(); curr.lock();
                  try {
                      if (!validate(pred, curr)) continue retry;
                      return curr.key == key;
                  } finally { curr.unlock(); pred.unlock();
                  }
              }
          }
          boolean add(int key, T item) {
          retry: while (true) {
                  Node pred = head, curr = pred.next;
                  while (curr.key < key) {
                      pred = curr; curr = curr.next;
                      if (curr == null) continue retry;
                  }
                  pred.lock(); curr.lock();
                  try {
                      if (!validate(pred, curr)) continue retry;
                      if (curr.key == key) return false; else {
                          Node node = new Node(key, item);
                          node.next = curr; pred.next = node;
                          return true;
                      }
                  } finally { curr.unlock(); pred.unlock(); }
              }
          }
          // remove аналогично
      }
    #+END_SRC
*** Ленивая синхронизация
    Как строить:
    1. Добавляем в ~Node boolean~ флажок, в котором будем помечать
       удаленные элементы. Удаление в две фазы -- флажок помечен
       соответствует логическому удалению, физическое следует позже.
    2. Инвариант: все непомеченные элементы всегда в списке.
    3. Результат: для валидации не надо просматривать список (только
       проверить, что элементы не удалены логически и ~pred.curr ==
       next~), остальное как в оптимистичном варианте.

    Поиск без ожидания:

    #+NAME:Ленивая синхронизация
    #+BEGIN_SRC java
      class Node {
          final int key;
          final T item;
          final Lock lock;
          boolean marked;
          // Очень важен volatile для линеаризуемости!
          volatile Node next;

          void lock() { lock.lock(); }
          void unlock() { lock.unlock(); }
      }

      class LinkedSet {
          boolean validate(Node prev, Node next) {
              return !pred.marked &&
                  !curr.marked &&
                  pred.next == curr;
          }

          boolean add(T elem) {
          retry: while (true) {
                  Node pred = head, curr = pred.next;
                  while (curr.key < key) {
                      pred = curr; curr = curr.next;
                      //                   ^^^^^^
                      //            тут curr.next != null
                  }
                  pred.lock(); curr.lock();
                  try {
                      if (!validate(pred,curr)) continue retry;
                      if (curr.key == key) {
                          curr.marked = true; // для validate
                          pred.next = curr.next; // точка линеаризации
                          return true;
                      } else return false;
                  } finally { curr.unlock(); pred.unlock(); }
              }
          }

          void delete (T elem) {
          retry: while (true) {
                  Node pred = head, curr = pred.next;
                  while (curr.key < key) {
                      pred = curr; curr = curr.next;
                      //                   ^^^^^^
                      //            тут curr.next != null
                  }
                  pred.lock(); curr.lock();
                  try {
                      if (!validate(pred,curr)) continue retry;
                      if (curr.key == key) return false;
                      else {
                          Node node = new Node(key, item);
                          node.next = curr; // сначала! порядок важен
                          pred.next = node; // тут точка линеаризации
                          return true;
                      }
                  } finally { curr.unlock(); pred.unlock(); }
              }
          }

          // Wait-free поиск!
          boolean contains(int key) {
              Node curr = head;
              while (curr.key < key) {
                  curr = curr.next; // точка линеаризации
              }
              return key == curr.key;
          }
      }
    #+END_SRC
*** Неблокирующая синхронизация
    Сделать синхронизацию без блокировок нетривиально:
    * Простое использование CAS не помогает -- удаление двух соседних
      элементов будет конфликтовать
      1, 2, 3, 4, удалим 2, 3 одновременно, но указатель 1 → 3
      сохранится.
    * Трюк такой: объединим ~(next, marked)~ в одну переменную, и будем ее
      изменять CASом атомарно.
      * Одновременное удаление соседних двух элементов будет
        конфликтовать
      * Каждая операция модификации выполняется одним успешным CAS'ом.
      * Это выполнение CAS'а и есть точка линеаризации
    * Будем пытаться удалять физически, от этого добавление и удаление
      станут lock-free, а поиск вообще wait-free.
    * В реализации будем использовать для пары
      ~java.util.concurrent.atomic.AtomicMarkableReference~.
** Продолжение построений на списках, стеках
   #+DATE: 2015.10.19
   Можно строить структуры универсально, храня на нее указатель и меняя
   его CAS'ом каждый раз. Так, например, работает счетчик -- в джаве
   это ~AtomicInteger~.

   Персистентные структуры тоже несложно пишутся, достаточно заменить
   CAS'ом root на новый после изменения структуры. Остальное дерево
   остается прежней версии (персистентность, собсна).
*** Стек LIFO
    Рассмотрим частный, вырожденный случай древовидной структуры --
    стек. Он не масштабируемый. Если конкуренция очень большая, то
    производительность в многосокетных системах на top будет падать.
    #+NAME: stack implementation
    #+BEGIN_SRC java
      // such immutable!
      class Node {
          final T item;
          final Node next;
      }

      final AtomicReference<Node> top = new AtomicReference<Node>(null);

      void push(T item) {
           while (true) {
                 Node node = new Node(item, top.get());
                 if (top.compareAndSet(node.next, node)) // линеаризация
                    return;
           }
      }

      T pop() {
        while (true) {
              Node node = top.get();

   if (node == null) throw new EmptyStack();
              if (top.compareAndSet(node, node.next)) // линеаризация
                 return node.item;
        }
      }
    #+END_SRC

    С разделяемой памятью вообще все достаточно сложно, там не только
    race condition'ы в большом количестве, но и куча проблем с
    производительностью. Будем пока считать что стек хороший.
*** Очереди на списках, Майкл-Скотт
    Будем делать очередь на списках. Наивно с помощью универсальной
    конструкции так себе, а популярный алгоритм -- Майкла Скотта.

    Делаем список, у очереди есть указатель на голову и хвост, все
    односвязно. Будем элементы добавлять и удалять достаточно
    естественно.
    Добавление: Создаем элемент, ссылаемся на голову, с помощью CAS'а
    меняем указатель на голову в классе.
    Дописать элемент в хвост сложно, потому что нужно поменять сразу две
    ячейки памяти -- указатель класса на хвост, указатель предыдущего
    элемента хвоста на последний.

    Идея алгоритма Майкла-Скотта такая: будем брать элемент и
    подписывать его в хвост, меняя ссылку предыдущего, а физически
    перемещать tail (указатель из класса) потом.
    Если другой поток увидит, что очередь в состоянии "есть ссылка на
    tail, у которого есть следующий элемент", то он может помочь
    переставить указатель класса на нужный элемент.

    #+NAME: Майкл-Скотт
    #+BEGIN_SRC java
      class Node {
          T item;
          final AtomicReference<Node> next;
      }

      AtomicReference<Node> head =
          new AtomicReference<Node>(new Node(null));
      AtomicReference<Node> tail =
          new AtomicReference<Node>(head.get());

      void enqueue(T item) {
          Node node = new Node(item);
       retry: while (true) {
              Node last = tail.get(),
                  next = last.next.get();
              if (next == null) {
                  if (!last.next.compareAndSet(null, node))
                      continue retry;
                  // оптимизация -- сами переставляем tail
                  tail.compareAndSet(last, node);
                  return;
              }
              // помогаем другим операциям enqueue с tail
              tail.compareAndSet(last, next);
          }
      }

      T dequeue() {
       retry: while (true) {
              Node first = head.get(),
                  last = tail.get(),
                  next = first.next();
              if (first == last) {
                  if (next == null) throw new EmptyQueue();
                  // Помогаем операциям enqueue с tail
                  tail.compareAndSet(last, next);
              } else {
                  if (head.compareAndSet(first, next)) // линеаризация
                      return next.item;
              }
          }
      }
    #+END_SRC
*** ABA problem
    Есть проблема в средах без сборки мусора, называется ABA. Суть:
    Будем реализовывать самый первый стек этой лекции на C, без Garbage
    collector'а.
    Добавим  в стек несколько элементов -- A и B.
    Может быть такое, что top стека может быть: A B A.
    Достанем указатель на top, сделаем успешно cas, на return нас
    перебил другой поток, и что-то переаллочилось, теперь в A лежит
    какая-то другая фигня.

    Еще раз: в стеке 1 элемент, по адресу A (top = A).
    Мы делаем ему pop, достаем A. В это время нас прерывают.
    Другой поток делает pop A, push B, pop B, push C на месте A появился
    другой элемент, но CAS сравнивает только указатели, и в этом случае
    он не обнаружит эту проблему.
    В джаве это не работает так, потому что память на A нельзя
    освободить, пока на нее ссылаются.

    Решить ABA проще всего с помощью реализации сборщика мусора.
    Другой способ -- пользоваться версиями. Хранить в top пару из
    указателя и версии. Таким образом если стек за время top.get и cas
    успел поменяться, мы сравним версии и упадем. Именно поэтому мы
    можем делать cas на 2х последовательных словах, это позволяет нам
    менять одновременно указатель + версию.
    Еще можно пользоваться Hazard Pointers -- многопоточный сборщик
    мусора, который работает только для наших узлов.
** Алгоритмы на массивах
*** Стек на массиве
    Давайте делать стек на массиве.

    В однопоточном варианте стек на массиве -- очень просто.
    Типа держим размер, pop/push меняет размер массива и ячейку.
    Но это все равно не взлетит в многопоточном варианте совсем прям
    наивно.

    Вот делаем мы ~push~. Сначала увеличим top cas'ом, а потом проставим
    элемент. Push будет работать, но pop в такой реализации упадет --
    если мы уже увеличили top, но не положили элемент, то достанет
    какой-то мусор.

    Аналогично если сначала проставляем элемент, а потом увеличиваем
    ~top~, то там будет что-то старое.

    С очередями проблемы те же.

    Будем писать дек, пытаясь реализовать obstruction-free свойство.
    Дек будет циклическим.  Храним в элементе пару -- значение и
    версия. Там где дек пустой, будем хранить ~(left_null, version)~,
    справа ~(right_null, version)~.

    Для корректности алгоритма не будем полагаться на указатели ~left~ и
    ~right~ в классе дека -- они будут типа для производительности, а
    индексироваться будем за $O(n)$.

    На практике этим никто не пользуется, потому что все равно
    медленнее, чем на ссылочном листе.

    #+NAME: Дек без помех
    #+BEGIN_SRC java
     int rightOracle() {
         int k = right; // для оптимизации
         while (a[k] != RN) k++;
         while (a[k-1] == RN) k--;
         right = k; // запомнили для оптимизации
         return k;
     }

     void rightPush(T item) {
      retry: while (true) {
             int k = rightOracle();
             {T item, int ver} prev = a[k-1], cur = a[k];
             if (prev.item == RN || cur.item != RN) continue;
             if (k == MAX-1) throw new FullDeque();
             if (CAS(a[k-1], prev, {prev.item, prev.ver+1} &&
                     CAS(a[k], cur, {item, cur.ver+1}))) return;
         }
     }

     T rightPop() {
      retry: while (true) {
             int k = oracleRight();
             {T item, int ver} cur = a[k-1], next = a[k];
             if (cur.itim == RN || next.item != RN) continue;
             if (cur.item == LN) throw new EmptyDeque();
             if (CAS(a[k], next, {RN, next.ver+1}) &&
                 CAS(a[k-1], cur, {RN, cur.ver + 1}))
                 return cur.item;
         }
     }
    #+END_SRC
*** Хэш-таблицы на массиве
    Бывают с прямой адресацией (по хэшу находим ведро, и все элементы с
    таким хэшом попадают в это ведро -- там дальше список или дерево).
    На практике с прямой адресацией все медленно, потому что там опять
    массивы или списки.
    Бывают с открытой, это самый лучший вариант.
    Но со списками намного проще.

    Будем пользоваться алгоритмом Split-Ordered lists.
    Засунем все элементы в одно большое связанео множество. Упорядочим
    их по хэшу. Для ускорения заведем слева хэш-таблицу, адресующую те
    элементы листа с заданным хэшом. Эта дополнительная таблица делается
    только для ускорения.
    Когда будем хотеть расширить таблицу, создадим вторую, скопируем ее
    черезстрочно, будем по мере обращений к хэшу ее обновлять (вторую).


    Открытая адресация.
    Делаем на массиве, будем считать ведро по хэшкоду, если занято, то
    дальше.
    Добавлять из нескольких потоков легко -- просто делаем cas. Удалять
    из такой таблицы можно прописывая некоторое особенное
    значение T. Нельзя прудмать алгоритм, который бы многопоточно
    закрывал дырки в этих списках.
    Ну, допустим мы забиваем элементы T, но как перевыделять память со
    временем -- для освобождения элементов T или расширения таблицы.

    Сделаем так, что таблица хранит указатель на "реальную" внутреннюю
    таблицу. Когда копируем, создаем новую таблицу, а указатель поставим
    в конце. Операция изменения ищет в новой таблице, если нету, то ищет
    в старой, если находит -- копирует в новую.
    Таким образом мы перенесем все элементы в новую таблицу.
    Как переносить, собственно?

    Если собираемся переносить, то пометим битиком значение. После этого
    мы занимаем слот в новой таблице, после этого копируем значение в
    новой таблице. Затем в старой пометим, что мы уже скопировали.
    #+BEGIN_SRC text
      (0, 0)
         ↓
      {Claim key}
         ↓
      (K, 0)
         ↓
      {Set value}
         ↓
      (K, V)            → Start copy → (K, V')
         ↕                               ↑
      {insert/delete}                  Moved
         ↕                               ↑
      (K, T)            → Moved      → (K, T')
    #+END_SRC
** CASN
   Этот алгоритм с переносом таблиц есть частный случай.
   Хотим чтобы работало корректно (линеаризуемо) и:
   1. Lock-free.
   2. Disjoint-Access Parallel (непересекающиеся доступы параллельны).

   #+NAME: CASN -- желаемое поведение
   #+BEGIN_SRC java
     boolean CASN(CASEntry... entries) atomic {
         for (CASEntry entry: entries)
             if (entry.a.value != entry.expect)
                 return false;
         for (CASEntry entry: entries)
             entry.a.value = entry.update;
         return true;
     }
   #+END_SRC

   Если мы сделаем CASN, то сделаем стек на массиве -- будем
   одновременно делать CAS 2 раза.

   #+NAME: CASN -- реализация
   #+BEGIN_SRC java
     import java.util.concurrent.atomic.AtomicReferenceFieldUpdater;

     public class CASEntry<T> {
         final DataReference<T> a; // что поменять
         final T expect; // ожидаемое значение
         final T update; // на что заменить
         // И тут простой конструктор для всех трех полей
     }

     // RDCSS сложна, только если ячейка может страдать от ABA.
     // Если нет, то проще.
     class RDCSSDescriptor {
         private final DataReference a1;
         private final Object expect1;
         private final DataReference a2;
         private final Object expect2;

         private final Object update2;
         // и конструктор

         Object invoke() {
             Object r;
             do {
                 r = a2.getAndCAS(expect2, this);
                 if (r instanceof RDCSSDescriptor)
                     ((RDCSSDescriptor)r).complete();
             } while (r instanceof RDCSSDescriptor);
             if (r == expect2) complete();
             return r;
         }

         void complete() {
             if (a1.value == expect1) a2.CAS(this, update2);
             else a2.CAS(this.expect2);
         }
     }

     enum Status {
         UNDECINED, SUCCEEDED, FAILED
     }

     class CASNDescriptor {
         private final DataReference status =
             new DataReference(Status.UNDECINED);
         private final CASEntry[] entries;

         // надо гарантировать одинаковый порядок обработки
         // DataReference каждым CASN, их надо как-то упорядочить
         CASNDescriptor(CASEntry[] entries) {
             this.entries = entries;
             Arrays.sort(this.entries);
         }

         boolean complete() {
             if (status.value == Status.UNDECINED) {
                 Status newStatus = Status.SUCCEEDED;
                 for (int i = 0; i < entries.length;) {
                     CASEntry entry = entries[i];
                     // AQUIRE ENTRY
                     Object val = new RDCSSDescriptor(this.status,
                                                      Status.UNDECIDED,
                                                      entry.a,
                                                      entry.expect,
                                                      this).invoke();
                     // AQUIRE ENTRY END

                     if (val instanceof CASNDescriptor) {
                         if (val != this) {
                             ((CASNDescriptor)val).complete();
                             continue; // retry this entry
                         }
                     } else if (val != entry.expect) {
                         newStatus = Status.FAILED;
                         break;
                     }
                     i++; // go to next entry
                 }
                 this.status.CAS(Status.UNDECIDED, newStatus);
             }
             boolean succeeded = status.value == Status.SUCCEEDED;
             for (CASEntry entry : entries) {
                 // RELEASE
                 entry.a.CAS(this, succeeded ? entry.update : entry.expect);
             }
             return succeeded;
         }
     }

     public class DataReference<T> {
         // хранимое значение
         volatile Object value;

         private static final
           AtomicReferenceFieldUpdater<DataReference, Object>
             VALUE_UPDATER =
               AtomicReferenceFieldUpdater.newUpdater(
                 DataReference.class, Object.class, "value");

         boolean CAS(Object expect, Object update) {
             return VALUE_UPDATER.compareAndSet(this, expect, update);
         }

         Object getAndCAS(Object expect, Object update) {
             do {
                 Object curval = value;
                 if (curval != expect) return curval;
             } while (!CAS(expect, update));
             return expect;
         }

         public T get() {
             while (true) {
                 Object curval = value;
                 if (curval instanceof RDCSSDescriptor) {
                     ((RDCSSDescriptor)curval).complete();
                     continue;
                 }
                 if (curval instanceof CASNDescriptor) {
                     ((CASNDescriptor)curval).complete();
                     continue; // retry
                 }
                 return (T)curval;
             }
         }

         public T get();
         public static boolean CASN(CASEntry... entries);
     }
   #+END_SRC
** Сложные блокировки
   Проведем анализ *конфликтов* (/data race/) -- два
   несинхронизированных доступа к одной ячейке данных, один из которых
   запись.

   *Матрица конфликтов* (для регистра) -- какие методы конфликтуют:
   |---+---+---|
   |   | R | W |
   |---+---+---|
   | R |   | × |
   |---+---+---|
   | W | × | × |
   |---+---+---|

   Подход этой матрицы позволяет чисто автоматизированно составить
   матрицу для сложной структуры с большим количеством методов.

   Можно тривиально убрать конфликты с помощью грубой блокировки на
   каждом конфликтующем методе.
   С другой стороны, жиненная ситуация -- после грубой блокировки
   некоторые методы могут работать одновременно (к примеру только
   читающие методы).

   Эту проблему решают read-write locks. Можем создать класс, который
   умеет лочиться по ~read~ или по ~write~. Такой класс будет принимать
   сколько угодно локов по ~read~, но остальные не будут совместимы.

   Другое решение -- делать структуру данных, используя тонкую
   блокировку. Например, с помощью CASN.

   Как сделать линеаризуемый многопоточный объект?
   1. Блокировки (aka synchronized): грубая, тонкая, ..., read-write.
   2. Без блокировки
      1. Универсальная конструкция (Copy-on-write + CAS, частичное
         копирование + CAS).
      2. CASN.
      3. Специфичные для структуры алгоритмы.

   Проблемы блокировки:
   1. В системе нет прогресса, пока объект заблокирован.
   2. Требуются дополнительные переключения контекста чтобы дать
      закончить работу блокирующему потоку. Это может сильно жрать
      CPU.
   3. Минимальный параллелизм работы, причем параллелизм обратно
      пропорционален количеству блокировок.
   4. Deadlocks.
** STM
   Как делать сложные вещи и не думать? STM!  Типа навешиваем какие-то
   вещи на кусок кода, и он выполняется атомарно. Такое есть, например,
   в Clojure. И в хаскеле тоже есть! Проблема -- оно работает медленно
   и поэтому не подходит для плюсов/джавы.

   #+NAME: Чего хочется от STM
   #+BEGIN_SRC java
     public class Employees {
         Set working = new ConcurrentSet();
         Set vacating = new ConcurrentSet();

         // псевдокод
         public boolean contains(Employee e) {
             atomic {
                 return working.contains(e) ||
                     vaccating.contains(e);
             }
         }

         public void startVacation(Employee e) {
             atomic {
                 working.remove(e);
                 vacating.add(e);
             }
         }
     }
   #+END_SRC

   Будем писать класс транзакций и класс переменной для транзакции.
*** Транзакции с блокировкой:
    * Можно двухфазовой блокировкой. Все конфликтующие блокировки
      защищаются локами, в начале транзакции локи накапливаются, в
      конце отпускаются.
    * Тогда любое исполнение такой системы будет линеаризуемо
    #+NAME: Реализация транзакций с блокировкой
    #+BEGIN_SRC java
      public class Transaction {
          private static final ThreadLocal<Transaction> CURRENT =
              new ThreadLocal<Transaction>();

          private final List<Lock> locks = new ArrayList<Lock>();

          private final Set<TVar<?>> writes = new HashSet<TVar<?>>();

          public void addWrite(TVar<?> var) {
              writes.add(var);
          }

          void addLock(Lock lock) { locks.add(lock); }

          // commit с блокировкой
          public boolean commit() {
              for (Lock lock : locks) lock.unlock();
              return true;
          }

          public void rollback() {
              for (TVar<?> var : writes) var.rollback();
              for (Lock lock : locks) lock.unlock();
          }

          public static Transaction beginTransaction() {
              Transaction t = new Transaction();
              CURRENT.set(t);
              return t;
          }

          public static Transaction currentTransaction() {
              return CURRENT.get();
          }

          public static <R> R atomic(AtomicBlock<R> call) {
              for (;;) {
                  Transacion t = beginTransaction();
                  try {
                      R result = call.call();
                      if (t.commit()) return result;
                  } catch (RuntimeException | Error e) {
                      t.rollback();
                      throw e;
                  }
              }
          }
      }

      public class TVar<T> {
          private T value;
          private final ReadWriteLock lock =
              new ReentrantReadWriteLock();

          // для rollback в Transaction
          private static final Object UNDEFINED = new Object();
          private Object oldValue = UNDEFINED;

          public T get() {
              lock.readLock().lock();
              Transaction.currentTransaction().addLock(lock.readLock());
              return value;
          }

          public void set(T value) {
              if (oldValue = UNDEFINED) {
                  lock.writeLock().lock();
                  this.oldValue = this.value;
                  Transaction.currentTransaction().addWrite(this);
              }
              this.value = value;
          }

          void rollback() {
              value = (T)oldValue;
              oldValue = UNDEFINED;
              lock.writeLock().unlock();
          }
      }
    #+END_SRC
*** Транзакции без блокировки
    Предоставим реализацию без помех. Разные потоки могут бесконечно
    долго мешать друг другу закончить транзакцию без прогресса, но если
    активен только один поток, то прогресс гарантирован.

    Проблематика -- даже читающие транзакции конфликтуют.  В этом
    смысле решение с блокировкой лучше.

    #+NAME: Реализация транзакции с блокировкой (obstruction-free)
    #+BEGIN_SRC java
      public class Transaction {
          private static final int ACTIVE = 0;
          private static final int COMITED = 1;
          private static final int ABORTED = -1;
          private final AtomicInteger state = new AtomicInteger(ACTIVE);

          public boolean isCommited() {
              return state.get() == COMMITED;
          }
          public boolean commit() {
              return state.compareAndSet(ACTIVE, COMMITED);
          }
          public void rollback() {
              state.compareAndSet(ACTIVE, ABORTED);
          }
          class VarHolder<T> {
              final Transaction owner;
              final Object value;
              Object newValue; // updated by owner

              VarHolder(Transaction owner, Object value) {
                  this.owner = owner;
                  this.value = value;
                  this.newValue = value;
              }

              // текущее значение зависит от состояния владельца
              T current() {
                  return owner.isCommited() ? (T)newValue : (T)value;
              }
          }
      }

      public class TVar<T> {
          private AtomicReference<VarHolder<T>> holder =
              new AtomicReference<VarHolder<T>>();

          public T get() {
              return (T)open().newValue;
          }

          public void set(T value) {
              open().newValue = value;
          }

          // переменную нужно открыть перед любым доступом
          VarHolder<T> open() {
              Transaction tx = Transaction.current();
              VarHolder<T> old, upd;
              do {
                  old = holder.get();
                  if (old.owner == tx) return old;
                  old.owner.rollback(); // если активен
                  upd = new VarHolder<T>(tx, old.current());
              } while (!holder.compareAndSet(old, upd));
              return upd
          }
      }

    #+END_SRC

    Параллельно читать можно, для этого необходимо в ~TVar~ при чтении
    не открывать переменную. Значение тогда сможет поменяться в
    процессе транзакции, и линеаризуемость пропадает.

    Решить это можно с помощью пост-проверки транзакции на
    корректность. Или с помощью многоверсионного контроля корректности.
** Мониторы и локи
   Представим операцию как функцию над парой из состояния и
   аргументов. Раньше мы рассматривали функции тлоько всюду
   определенные.

   Возьмем блокирующую очередь. Пусть ~put~ кладет только, если есть
   место. Если нету, то она зависает, то есть put частично
   определена. Аналогично предтсавим себе take, который может
   вытаскивать элемент из очереди только, если очередь не пуста. Будем
   поддерживать, с другой стороны, и не блокирующиеся операции -- ~size~,
   ~offer~, ~poll~ (возвращает ~null~ если пуста).

   Примечание: тут блокировка обозначает нечто другое -- определенность
   функции.

   Тут нужно переопределить линеаризуемость и исполнение:
   1. $inv(A)$ -- это вызов, но не всегда есть $resp(A)$. $A$ называется
      незавершенной операцией, а $inv(A)$ незавершенным вызовом.
   2. Исполнение линеаризуемо, если в исполнении можно:
      * Добавить такие ответы для незавершенных вызовов.
      * Выкинуть остальные незавершенные вызовы.
      * Можно упорядочить, получить допустимое последовательное
        исполнение: \[inv(A₁) → resp(A₁) → ⋯ \]

   *Монитор* -- это пара из mutex'а и набора условных переменных:
   1. Взаимное исключение для защиты данных от одновременного изменения.
   2. Условные переменные для ожидания.
   3. Придумано Энтони Хоаром.

   В java каждый объект имеет монитор с одной условной переменной:
   * ~synchronized == monitorenter + monitorexit.~
   * ~wait~, ~notify~, ~notifyAll~ -- для работы с условной переменной.

   Что такое wait?
   * Может выходить из критической секции (монитора), чтобы другие
     потоки могли в нее попасть и поменять состояние объекта
   * Дожидается сигнала через условную переменную.
   * Снова входит в критическую секцию (в монитор), чтобы этот поток
     мог перепроверить состояние объекта и выполнить свою операцию если
     состояние подходящее.
   * Сигнал посылается через ~notify~ (сигнал одному ждущему потоку),
     ~notifyAll~ (сигнал всем ждущим потокам).

   #+NAME: Пример очереди в java
   #+BEGIN_SRC java
     public class BlockingQueue<T> {
         private final T[] items;
         private final int n;
         private int head;
         private tail;

         public synchronized int size() {
             return (tail - head + n) % n;
         }

         // Если очередь пуста, возвращает null
         // полностью определен в любом состоянии
         public synchronized T poll0 {
             if (head == tail) return null;
             T result = items[head];
             items[head] = null;
             head = (head + 1) % n;
             return result;
         }

         // не определен для пустой очереди
         // Если очередь пуста -- ждет. Кидает exception == может блокироваться.
         // Цикл зачем? См. Object.wait: spurious wakeups are possible...
         public synchronized T poll throws Interruptedexception {
             while (head == tail) wait(); // критическая разница
             T result = items[head];
             items[head] = null;
             if ((tail + 1) % n == head) notifyAll(); // очередь была полна
             head = (head + 1) % n;
             return result;
         }

         // Сам метод не блокируется, но будит потоки, которые ждут
         // пока очередь станет не пуста
         // Нужно будить другие потоки, только если действительно
         // очередь становится не пуста.
         // Сигнал пойдет только после выхода из монитора (критической секции).
         public synchronized boolean offer(T item) {
             int next = (tail + 1) % n;
             if (next == head) return false;
             items[tail] = item;
             if (head == tail) notifyAll();
             tail = next;
         }

         // ждет пока очередь не полна и будет потоки, которые могут ждать пока
         // очередь станет не пуста
         public synchronized void put(T item) throws Interruptedexception {
             while (true) {
                 int next = (tail + 1) % n;
                 if (next == head) { wait(); continue; }
                 items[tail] = item;
                 if (head == tail) notifyAll();
                 tail = next;
                 return;
             }
         }

         // в методе take тоже нужно пытаться будить put, когда мы забрали последний
         // элемент
     }
   #+END_SRC

   Рассмотрим еще раз разницу ~notify~ и ~notifyAll~:
   * Нам нужно было использовать одну условную пееменную для двух
     условий: очередь не пуста и очередь не полна, поэтому пользуемся
     ~notifyAll~.
   * Если бы для каждого условия использовалась бы отдельная
     переменная, ~notify~ было бы достаточно. Но у java есть только
     одна условная переменная на каждый монитор.

   ~j.u.c.ReentrantLock~ спасает! Там есть методы всякие, которые
   предоставляет интерфейс ~Condition~ с методами ~await~, ~signal~,
   ~signalAll~. Можно таким образом сделать эффективным ~take~, в котом мы
   делаем все то же самое, что с интерфейсом ~wait/notify~, но на локе и
   методами с похожими названиями. Но тут можно сделать два condition'а
   и делать на каждом ~signal~, а не ~signalAll~.

   У каждого потока есть флаг ~interrupted~.
   1. Его ставит метод ~Thread.interrupt~.
   2. Его проверяют методы ~wait~, ~await~ и так далее.
   3. В случае обнаружения выставленного эти методы:
      * Прекращают ждать.
      * Сбрасывают флаг.
      * Кидают ~InterruptedException~.

   Что делать с ненужным ~InterruptedException~? Если нужно
   реализовывать метод, который ждет, но не кидает
   ~InterruptedException~, то ~interrupted~ флаг надо
   перевыставить. Тогда ожидание можно прерывать через
   ~Thread.interrupt~.

   Частая ошибка в ббилиотеках -- забыли перевыставить ~interrupted~
   флаг.

   #+BEGIN_SRC java
     // возвращает null если прервали InterruptedException
     public T takeOrNull() {
         try {
             return take();
         } catch (InterruptedException e) {
             // перевыставим флаг interrupted
             Thread.currentThread().interrupt();
             return null;
         }
     }
   #+END_SRC

   Пишем поток, обрабатывающий очередь.
   1. Заводим свой флаг, сигнализирующий что поток надо остановить. В
      отличии от флага ~interrupted~, нет риска что какой-то сторонний
      метод его случайно сбросит.
   2. для прерывания ожиданий нужен ~Thread.interrupt()~. *НИКОГДА* не
      пользоваться ~Thread.stop()~.

   Главный метод: метод run выполняется в отдельном потоке. Метод run
   выходит в случае прерывания.
** SPSC очередь без блокировок и конвейер
   Есть задачи и последовательные действия. $A₁...Aₙ$ (типа посчитать
   что-нибудь, преобразовать ответ, запаковать, записать в файл,..).

   Пусть время выполнения действия $Aᵢ$ равно $tᵢ$. Тогда общее время
   на задачу равно $∑tᵢ$. Один поток в единицу времени выполняет
   $\frac{1}{∑tᵢ}$ задач.

   Для увеличения пропускной способности сделаем конвейер на $n$
   потоках.

   Структура SPSC очереди.

   Не блокирующийся ~offer~: пишем без блокировок, поэтому важен порядок
   действий и точки линеаризации операций.
   CAS не нужен, только один producer меняет ~tail~. Ожиданием займемся позже.
   #+BEGIN_SRC java
     public boolean offer(T item) {
         // читаем один раз tail (только мы его меняем)
         int tail = this.tail;
         // здесь volatile чтение head (его меняет consumer)
         if (((tail+1) & mask) == head) return false; // полна
         items[tail] = item;
         // в самом конце передвинем tail
         this.tail = (tail + 1) & mask; // volatile write
         //        ^ это точка линеаризации операции offer
         return true;
     }
   #+END_SRC

   * ~LockSupport.park~ усыпляет текущий поток до тех пор:
     * Пока другой поток не вызовет ~unpark~
     * Что-то еще...
   * ~LockSupport.unpark~ делает ???

   Вот ~offer~ с ~unpark~:
   #+BEGIN_SRC java
     public boolean offer(T item) {
         int tail = this.tail;
         if (((tail+1) & mask) == head) return false; // полна
         items[tail] = item;
         this.tail = (tail + 1) & mask; // volatile write
         LockSupport.unpark(consumer); // разбудить ждущего потребителя
         return true;
     }
   #+END_SRC

   #+BEGIN_SRC java
     public T take() throws InterruptedException {
         // это поможет при отладке
         assert Thread.currentThread() == consumer;
         // читаем один раз head
         int head = this.head; // volatile read
         while (true) {
             if (Thread.interrupted()) throw new InterruptedException();
             // здесь volatile чтение tail ( его меняет producer)
             if (head == tail) { LockSupport.park(); continue; }
             T result = items[head];
             items[head] = null;
             this.head = (head + 1) & mask;
             LockSupport.unpark(producer);
             return result;
         }
     }
   #+END_SRC

   Блокирующийся ~take~ -- разбор:
   * Нужен цикл ожидания (park может проснуться сам).
   * Нужно избежать ухода в бесконечный цикл.

   Блокирующийся ~take~ -- оптимальный ~unpark~.
   #+BEGIN_SRC java
     public T take() throws InterruptedException {
         // это поможет при отладке
         assert Thread.currentThread() == consumer;
         // читаем один раз head
         int head = this.head; // volatile read
         while (true) {
             if (Thread.interrupted()) throw new InterruptedException();
             // здесь volatile чтение tail ( его меняет producer)
             if (head == tail) { LockSupport.park(); continue; }
             T result = items[head];
             items[head] = null;
             this.head = (head + 1) & mask; // volatile write
             //        ^ это точка линеаризации операции take
             // если очередь была полна до операции (producer мог спать)
             if (((takl + 1) & mask) == head) LockSuppor.unpark(producer);
             return result;
         }
     }
   #+END_SRC

   * Все остальные операции аналогично.
   * Оптимизации SPSC очереди.
     * Блочная работа - можно доставать сразу несколько задач.
     * Обобщается на конвейер из n потоков.
       * $n$ потоков, работающие в конвейере, будут использовать общий
         циклический буфер.
       * Кладем задачу в буфер первым действием
       * Удалем задачу из буфера последним действием
       * У каждого потока есть свой ~index~ в буфере, а с $n = 2$ было
         ~tail == producer index, head == consumer index~.
       * Каждый поток работает над задачами до ~index~ предыдущего потока
         в конвейере и следит, чтобы не упереться в следующий.

   Практические наблюдения про конвейеры:
   * *Конвейер* (/pipeline/) имеет смысл, если отдельные действия по
     задаче примерно равны по продолжительности
   * Есть накладный расход на организацию. На быстрых действиях не выгодно.
   * Накладной расход на задачу можно уменьшить, обрабатывая элемениты
     пачками (/batching/).
   * Конвейер повышает *пропускную способность* (/throughput/) принося
     в жертву *задержку* (/latency/) -- время обработки одной задачи от
     начала до конца.
* Распределенные системы
** Билеты
   1. Логические часы Лампорта и векторные часы, их свойства
   2. Часы с прямой зависимостью (и их свойства) и матричные часы.
   3. Взаимное исключение в распределенной системе. Централизованный
      алгоритм.
   4. Взаимное исключение в распределенной системе. Алгоритм Лампорта,
   5. Взаимное исключение в распределенной системе. Алгоритм Рикарта и
      Агравалы.
   6. Взаимное исключение в распределенной системе. Алгоритм обедающих
      философов.
   7. Взаимное исключение в распределенной системе. Алгоритм на основе
      токена.
   8. Взаимное исключение в распределенной системе. Алгоритмы основе
      кворума (простое большинство, рушащиеся стены).
   9. Согласованное глобальное состояние (согласованный
      срез). Алгоритм Чанди-Лампорта. Запоминание сообщений на стороне
      отправителя.
   10. Согласованное глобальное состояние (согласованный
       срез). Алгоритм Чанди-Лампорта. Запоминание сообщений на
       стороне получателя.
   11. Глобальные свойства. Стабильные и нестабильные
       предикаты. Слабый конъюнктивный предикат. Централизованный
       алгоритм.
   12. Слабый конъюнктивный предикат. Распределенный алгоритм.
   13. Диффундирующие вычисления. Останов. Алгоритм Дейксты и Шолтена.
   14. Локально-стабильные предикаты, согласованные интервалы,
       барьерная синхронизация (3 алгоритма). Применение для
       определения взаимной блокировки.
   15. Упорядочивание сообщений. Определения, иерархия
       порядков. Алгоритм для FIFO.
   16. Упорядочивание сообщений. Определения, иерархия
       порядков. Алгоритм для причинно-согласованного порядка.
   17. ? Упорядочивание сообщений. Определения, иерархия
       порядков. Алгоритм для синхронного порядка.
   18. Общий порядок (total order). Алгоритм Лампорта.
   19. Общий порядок (total order). Алгоритм Скина.
   20. Иерархия ошибок в распределенных системах. Отказ узла в
       асинхронной системе --- невозможность консенсуса
       (доказательство Фишера-Линча-Патерсона).
   21. Консенсус в распределенных системах. Применение консенсуса:
       выбор лидера, terminating reliable broadcast.
   22. Синхронные системы. Алгоритм для консенсуса в случае отказа
       заданного числа узлов.
   23. Синхронные системы. Проблема византийских генералов. Алгоритм
       для N >= 4, f = 1. Объяснить идею обобщения для f > 1.
   24. Синхронные системы. Проблема византийских
       генералов. Невозможность решения при N = 3, f = 1.
   25. Недетерминированные алгоритмы консенсуса. Алгоритм Бен-Ора.
   26. Paxos. Алгоритм, его свойства. Основные модификации.
   27. Транзакции в распределенных системах. 2 Phase Locking, 2 Phase
       Commit.
** Вступление
   #+DATE: 2016-02-15
   Чем отличается курс от предыдущего? Типа будем изучать модели,
   которые обмениваются сообщениями. Это можно сэмулировать на том,
   что мы проходили в предыдущем курсе, но это неэффективно. А хочется
   быстро, типа как в ~NUMA~ там или вот эти все ~MOESI~ (но
   когерентные кеши -- это параллельная система, а не распределенная).

   Все короче распределенное в eтом мире поетому предмет оче важный и
   интересный!

   Система масштабируется вертикально, если мы даем ей больше ядер и
   она быстрее работает. Система масштабируется горизонтально, если
   от большего количества машин становится лучше.

   Важные отличия распределенных систем:
   1. Нельзя полагаться на общее время или общее состояние системы --
      оно постоянно меняется.
   2. Географически распределена.
   3. Обмен сообщениями, а не общая память.
   4. Может отказать частично.
   5. Больше надежность, сложно кодить, меньше стоит.

   Модель:
   * Проессы $P,Q,R ... ∈ ℙ$
   * События $a,b,c,d,e... ∈ E$, в процессах $proc(e) ∈ P$.
   * Сообщения $m ∈ M$, события посылки/приема $snd(m),rev(m) ∈ E$.
   * Произошло-до между событиями ($→$):
     * Минимальный строгий частичный порядок, что
       * если $e$ и $f$ произошли в одном процессе и $e < f$ (e шло перед
         f), то $e → f$.
       * если $m$ сообщение, то $snd(m) → rcv(m)$.
     * Другими словами -- транзитивное замыкание порядка событий на
       процессе и посылке/приема сообщений.

   Графическая нотация -- параллельные горизонтальные линии как
   потоки, стрелочки между узлами на них как передчи сообщений, вот
   ето все.

   Логические часы: для каждого события e определим число $C(e)$ так,
   что: $∀e,f ∈ E:e → f ⇒ C(e) < C(f)$.

   Такая функция $C$ называется логичксеими часами. В обратную сторону
   отношение не верно и не может быть, потому что порядок на числах
   полный, а произошло-до -- частичный.

   *Логические часы Лампорта*: время это функция $C(e)$, где $e$ --
   событие, в каждом процессе.

   Алогоритм:
   * Перед каждой посылкой увеличивает $C$ на единицу.
   * При посылке сообщения процесс посылает свое время $C$.
   * При приеме сообщения делаем $C := max(received_C, C) + 1$.

   Очевидно логические часы Лампорта являются логическими часами.
   События в одинаковое логическое время параллельны.

   *Векторные часы*: для каждого события e определим вектор $VC(e)$ так,
   что $∀e,f ∈ E: e → f ⇔  VC(e) < VC(f)$.

   Сравнение векторов происходит покомпонентно (не лексикографически).
   Такая функция будет называтеся векторными часами.

   Алгоритм векторного времени: время это целочисленный вектор
   $VC$. Будем по посылке инкрементировать, по посылке отправлять, а при
   приеме делаем покомпонентно $VC: = max(received_VC, VC)$.

   При этом важно: $∀e,f ∈ E: proc(e) = Pᵢ, proc(f) = Pⱼ : e → f ⇔
   (VC(e)ᵢ,VC(e)ⱼ) < (VC(f)ᵢ, VC(f)ⱼ)$. Типа хватает только двух
   сравнений. Если процессы параллельны, то вектора будут несравнимы.

   *Часы с прямой зависимостью* (/direct dependency/): храним вектор, а
   посылает только одно число: $∀e,f ∈ E: e →_d f ⇔ VC_d(e) <
   VC_d(f)$, где $e →_d f ⇔ e < f ∨ ∃m ∈ M: e ≤ snd(m) ∧ rcv(m) ≤
   f$. Менее формально, $e →_d f$ значит произошло-до суженное на один
   процесс либо на два процесса $P$ и $Q$ ($e ∈ P, d ∈ Q$, соединенных
   сообщением от $Q$ к $P$).

   Формально это комбинация Лампорта и векторного.

   *Матричные часы*: храним матрицу, посылаем тоже матрицу. В отличии
   от векторных часов, дополнительное пространство позволяет процессу
   $M$ знать, что процесс $P$ знает о процессе $Q$ (в то время, как
   векторные часы представляют собой знание процесса $M$ о процессе
   $P$). Скажем, если $∀i, s.[i,s.p] > k$, то процесс $s.p$ может
   утверждать, что все знают, что его состояние больше $k$.

   *Взаимное исключение* в распределенных системах:
   1. Критическая секция $CSᵢ$ состоит из двух событий:
      * $Enter(CSᵢ)$ -- вход
      * $Exit(CSᵢ)$ -- выход
      * $i$ -- порядковый номер захода в критическую секцию
   2. Основное требование: *взаимное исключение*: два процесса не
      должны быть в критической секции одновременно, то есть $Exit(CSᵢ)
      → Enter(CSᵢ₊₁)$.

   Также в системах с общей памятью нужны доп требования /прогресса/:
   * Минимальное требование -- каждое желание процесса попасть в
     критическую секцию будет рано или поздно удовлетворено.
   * Также может быть гарантирован тот или иной уровень честности
     удовлетворения желаний процессов о входе в крит. секцию.


   Алгоритмы взаимного исключения (хотим что-то делать в критической
   секции):
   1. Централизованный алгоритм:
      * Выделенный координатор
      * Три типа сообщений:
        * ~req[uest]~ -- от запрашиваающего процесса координатору
        * ~ok~ -- от координатора
        * ~rel[ease]~ -- после выхода из критической секции
      * Требует 3 сообщения на критическую секцию независимо от
        количества участвующих процессов
      * Не масштабируется из-за необходимости иметь выделенного
        координатора.
      * Подход шаринга -- это короче сделать несколько координаторов и
        пользоваться ими. Но это не решает проблему надежности -- если
        падает один координатор, то его часть данных, за которую он
        отвечает, становится недоступна.
   2. Алгоритм Лампорта ($3N-3$ сообщения). Устроен как
      централизованый, но на основе логических часов (векторных с
      прмяой зависимостью).

      Каждый процесс хранит очередь всех билетиков на запрос, то есть
      некоторую очередь.

      Процесс, который хочет зайти в критическую секцию, добавляет
      себя в конец своей очереди и посылает ~req~ всем другим
      процессам. Процесс, принимающий ~req~ делает следующее: если он
      принимает два ~req~ с одинаковыми часами, то упорядочиваем их по
      номеру процесса. Первому запросу отвечают ~ok~ с меткой времени.

      Когда процесс получает ~ok~ от всех, то он входит в критическую
      секцию, если его запрос всем остальным был сделан раньше. Типа
      отправил раньше чем все другие обработали.

      После выхода из критической секции всем посылается ~rel~. Те,
      кто получают ~rel~, удаляют соответствующий вектор из очереди.

      Этот алгоритм сильно основан на FIFO: любые два сообщения от
      одного потока другому должны придти в порядке отправления.
   3. Алгоритм Рикарда и Агравалы ($2N-2$ сообщения, starvation free).

      Модификация алгоритма Лампорта: давайте не будем отсылать ~ack~
      на ~req~ если мы работаем в критической секции. Объединим
      acknowledge и release в одно сообщение ~okay~. Когда процесс
      хочет войти в критическую секцию, он рассылает всем ~okay~ и
      ждет, пока все ему напишут ~okay~.

      По получении ~okay~ от кого-то:
      1. Если ничего не хочет -- отправляет ~okay~ обратно.
      2. Если уже послал запрос -- сравнивает свой запрос (метку
         времени) с пришедшей (+ еще сравнение по номеру потока) и
         если пришедшая больше (позже), отвечает ~okay~.
      3. Иначе задерживает ответ (кладет в очередь).

      После выхода из критической секции рассылает все задержанные
      отклики.
   4. Алгоритм обедающих философов (от $0$ до $2N-2$ сообщений).
      Проблема предыдущих алгоритмов в том, что они посылают сообщения
      потокам, которые потенциально могут вообще не хотеть войти в
      критическую секцию.

      Пусть есть $n$ процессов и между соседними есть какие-то
      конфликты памяти (в задаче философов граф конфликтов -- колесо, а
      в общем случае граф может быть любым). Ориентируем граф
      конфликтов так, чтобы в нем не оказалось циклов. Если нет
      входящих ребер (мы -- исток), тогда будем брать ресурсы. После
      использования ресурсов все исходящие ребра перевернем. Две
      теоремы из теории графов, которые гарантируют корректность:
      1. в графе без циклов можно истоки инвертировать и циклов не
         образуется.
      2. в графе без циклов есть исток.

      Ресурсы могут быть чистыми и грязными. Задача -- собрать все
      чистые ресурсы. Грязные ресурсы получаются после
      использования. Если приходит запрос на грязный ресурс, мы его
      чистим и отдаем.

      Как инициализировать граф? У потоков есть идентификаторы. Можно
      посортить с помощью них -- граф отсортированный таким образом не
      будет иметь циклов (топсорт).

      Можно решить эту задачу и рандомно -- хватать какие-то ресурсы,
      если не получилось, отдаем и ждем какое-то рандомное количество
      времени.
   5. Алгоритм на основе токена.

      Работает в системах, где связь кольцевая. Процесс будет выполнять
      критическую секцию только когда у него есть токен. После
      использования токена или если делать ничего не нужно, токен
      отправляется по кольцу дальше.

      Как раз ethernet работает как рандомизированные философы с полным
      графом, а token ring вот делал кольцевую передачу данных,
      отцепляя от информации ту, которая нужна конкретно одному
      компуктеру.
   6. Алгоритмы на основе кворума.

      Будем спрашивать только подмножество процессов. Кворум: $Q ⊂ 2ᵖ$:
      $∀A,B ∈ Q: A∩B ≠ ∅$. Также кворум должен быть замкнут по
      надмножеству.

      * Централизованный кворум -- одна машина.
      * Простое большинство (вес каждого узла единица) и взвешенное
        большинство (веса произвольны).
      * Рушащиеся стены -- упорядочиваем все процессы в прямоугольник
        (по возможности), тогда кворумом является любая прямая и по
        одному представителю из каждой другой параллельной линии.
** Глобальные состояния и срезы
   Хотим уметь запоминать состояние распределенной системы, чтобы
   можно было ее восстановить в случае каких-либо проблем.

   * *Срез* -- такое $G ⊂ E$, что $∀f ∈ G, e ∈ E, e < f ⇒ e ∈ G$.
   * *Согласованный срез* -- срез $G$, что $∀f ∈ G, e ∈ E: e → f ⇒ e ∈
     G$.

   [[http://neerc.ifmo.ru/wiki/index.php?title=Алгоритм_Чанди-Лампорта][Алгоритм Чанди Лампорта на neerc.ifmo.ru]]

   *Алгоритм Чанди-Лампорта* для согласованного запоминания глобального
   состояния системы (получения согласованного среза):
   1. Есть инициатор (observer), который запоминает свое состояние
      (становится красным), посылает токен всем другим потокам.
   2. При получении токена /первый раз/ процесс запоминает свое
      состояние (становится красным), посылает свое состояние
      инициатору и в дальнейшем прикрепляет красный маркер к всем
      сообщениям.
   3. Чтобы восстановить работу системы из запомненного состояния
      (/checkpoint/) надо еще запоминать сообщения в пути.
      Запоминание сообщений: нужно запомнить все сообщения $m$ такие
      что $snd(m) ∈ G ∧ rcv (m) ∈ E \ G$, где $G$ -- согласованный
      срез.
   4. Когда красный процесс получает сообщение от белого процесса (без
      токена), то процесс форвардит сообщение инициатору (или хранит у
      себя). Это называется запоминание *на стороне получателя*.
   5. *На стороне отправителя*: пока ты белый, запоминаешь все посланные
      запросы. Если кто-то отправил на запрос ack, то сообщение можно
      удалять. Когда кто-то становится красным, то он отправляет
      отправителю всех сообщений, что хранит, красный ack, и тот их
      удаляет из себя.
   6. Запомненные состояния образуют согласованный срез, если
      сообщения между процессами идут FIFO. В ином случае с сообщением
      следует отправлять цвет отправителя, а белых процессов обязать
      становиться красными при получении сообщения от красного.

   *Глобальные свойства*:
   * Стабильные предикаты
     * Берем согласованные срезы; если предикат верен, то будет верен
       и в дальнейшем. С другой стороны, строить согласованный срез
       дорого -- $O(n²)$ сообщений.
     * Пример -- потеря токена, взаимная блокировка,..
   * Нестабильные предикаты
     * Локальный предикат -- предикат по состоянию одного процесса.
     * Если предикат есть дизъюнкция локальных предикатов, то
       вычисление легко, достаточно найти хотя бы один верный.
     * С конъюнкцией все неочевидно -- как найти правильный срез?

   Если предикат $P$ имеет вид конъюнкции локальных предикатов над
   состоянием каждого процесса: $P = L₁∧L₂∧...∧Lₙ$, и его истинность
   есть истинность хотя бы на одном срезе, то такой предикат
   называется слабым конъюктивным. Как пример
   предиката -- "в системе нет координатора", причем локальное условие
   -- "я не координатор".

   Сложные предикаты, являющиеся логической комбинацией локальных
   предикатов, всегда можно представить в нормальной дизъюнктивной
   форме и рассматривать как дизъюнкцию слабых конъюнктивных
   предикатов.

   Слабый конъюнктивный предикат: *централизованный алгоритм*.
   * Каждый работающий процесс отслеживает свое векторное время $VC$.
   * При наступлении истинности локального предиката $L$ увеличиваем
     свою компоненту, посылаем сообщение координатору $C$, указывая
     векторное время когда это произошло.
   * Любой срез итого задается вектором.
     * Координатор поддерживает в памяти /срез-кандидат/ и очередь
       необработанных сообщений от каждого процесса.
     * Срез кандидат согласован тогда и только тогда когда все
       соответствующие вектора попарно несравнимы.
   * Координатор хранит вектора среза-кандидата и флажок для каждой
     его компоненты: красный -- этот элемент не может быть частью
     согласованного среза, зеленый -- может.
   * Начальное состояние -- все по нулям, красные.
   * Обрабатываем приходящие сообщения только от красных процессов,
     сообщения от зеленых ставим в очередь.
     * Сравниваем пришедший вектор попарно с другими процессами, если
       новый вектор больше, то делаем меньший процесс красным.
     * После обработки сообщения делаем процесс зеленым.
   * Если все зеленое, то мы нашли согласованный срез.

   *Распределенный алгоритм* для проверки слабого конъюнктивного
   предиката:
   * Каждый процесс имеет своего собственного координатора
   * Процессы шлют те же сообщения своим координаторам.
   * Координаторы пересылают друг другу срезы-кандидаты и флажки
     (отсылают информацию о своем процессе, запоминают информацию о
     других).
   * Красные координаторы (координаторы красных процессов)
     обрабатывают сообщения от своих процессов.
     * После обработки сообщение становится зеленым.
     * Если в процессе обработки они пометили красным другой процесс,
       то шлют сообщение его координатору.
   * Если координатор замечает момент, в котором ему рассказали о всех
     зеленых чуваках и его парень тоже зелен, и никто не сравним --
     тогда он победил.
** Диффундирующие вычисления
   * Процесс-environment, начинает вычисление
   * Процессы в системе активные или пассивные, можно стать активным
     только по получении сообщения. Стать пассивным можно всегда.
   * Только активные процессы могут слать сообщения.

   Алгоритм Дейкстры-Шолтена -- способ определить, что диффундирующее
   вычисление завершено. Будем думать, что каждый порожденный активный
   процесс сообщением есть сын того, кто его разбудил. Тогда в общем
   случае, если граф -- дерево:
   1. Cыновья должны пинговаться о том, что они живы.
   2. Если процесс ничего не считает, он отвечает на запрос
      отрицательно.
   3. Когда процесс не имеет детей и ничего не считает, он оповещает
      родителя об этом.
   4. Когда корень дерева в ситуации 3, вычисление окончено.

   Алгоритм логично продолжается на ациклические графы, надо хранить
   количество запросов, которые в тебя поступили и количество
   ответов. Когда разность ноль -- говорим всем, что отключаемся.
** Локальная стабильность, баръерные синхонизации
   * *Локально-стабильный* предикат -- такой, что если он выполняется
     для какого-то среза, то и дальше тоже выполняется.
   * Интвервал -- пара срезов $[X, Y]$, таких что $X ⊂ Y$.
   * Пусть $G$ и $H$ срезы, тогда $[X, Y]$ согласован, если $∀e,f : (f
     ∈ X) ∧ (e → f) ⇒ e ∈ Y$. Все, что случилось до любого события из
     $X$ есть в $Y$. Иное определение согласованности интервала, что
     $∃G$ -- согласованый, такой что $X ⊂ G ⊂ Y$.
   * $[X,Y]$ баръерно-синхронизирован, если $∀g ∈ X ∧ h ∉ Y: g →
     h$. Интуитивно, каждое событие в $X$ было выполнено перед каждым
     событием не из $Y$. Короче это каким-то образом мы сжали
     happens-before, так что все прошлое связано с будущим.
   * Достижение баръерной синхронизации (будем искать такие $X$ и $Y$):
     1. По кольцу -- сверху до низу и обратно.
     2. Все всем сразу.
     3. Все координатору, потом координатор всем.

   Чтобы проверить, что локально-стабильный предикат выполняется,
   достаточно взять любые два среза до $X$ и после $Y$ и сравнить.

   Детектить дедлоки можно так: каждый процесс знает, кого ждет. Также
   у каждого процесса есть $bool$ переменная $changed$, которая
   означает, поменялись ли ожидания с предыдущего запроса. Координатор
   опрашивает каждый поток, спрашивая, кого он ждет, строит граф. Если
   видет в нем цикл, повторяет еще раз, спрашивая, поменялось ли
   что-то. Если не поменялось, то дедлок. Поскольку координатор
   спрашивает всех два раза, он делает баръерную синхронизацию.
** Упорядочивания событий
   Какие есть упорядочивания событий:
   * Для обычных сообщений между парой процессов (/unicast/):
     * Асинхронно (нет порядка).
     * FIFO (/first in first out/):

       $∄m,n ∈ M : snd(m) < snd(n) ∧ rcv(m) > rcv(n)$

       Алгоритмы FIFO используют нумерацию сообщений. Типа если надо
       сделать FIFO, то заставим всех отправляющих нумеровать
       сообщения, а всех принимающих смотреть на эти номерки и
       шафлить, если вдруг пришло не по очереди.
     * Причинно-согласованный порядок (/casually consistent order/).

       $∄m,n ∈ M : snd(m) → snd(n) ∧ rcv(n) → rcv(m)$

       Используется отношение "произошло до". Алгоритм для достижения
       такого порядка основан на матричных часах. Пусть мы поток номер
       $i$. Заведем матрицу $M[i,j]$, заполненную нулями. Перед
       отправлением потоку $j$ будем делать $M[i,j] := M[i,j] + 1$. И
       прикладывать новую матрицу. При получении матрицы $W$ от $P_j$,
       будем считать что сообщение нам подходит, если:

       $(W[i,j] = M[i,j] + 1) ∧ (∀k ≠ j : M[k,i] ≥ W[k,i])$

       В этом случае принимаем сообщение и делаем $M := max(M,W)$.

     * Синхронный порядок (/synchronous order/) -- всем событиям можно
       сопоставить время $T(m)$ так что время событий

       $T(rcv(m)) = T(snd(m)) = T(m)$ и $∀e,f ∈ E: e → f ⇒ T(e) < T(f)$.

       Алгоритм основан на иерархии процессов.

     * Суммарно для unicast'а имеем: Synchronious $⊂$ Casually ordered
       $⊂$ FIFO $⊂$ Asynchronious
   * Multicast/broadcast -- общий порядок (/total order/). Только для
     случаев когда одно сообщение идет многим получателям ($rcv$ с
     индексом процесса).

     $∄m,n ∈ M; p,q ∈ P: rcvₚ(m) < rcvₚ(n) ∨ rcv_q(n) < rcv_q(m)$

     Тривиально выполняется для всех unicast сообщений.

     Алгоритмы общего порядка -- Лампорта и Скина.

     * *Лампорт*. Модифицируем алгоритм взаимного исключения Лампорта,
       предполагая FIFO-порядок. Сообщения транслируются всем
       процессам. Каждый процесс содержит логические часы и очередь
       отправленных сообщений.

       1. Чтобы отправить сообщение, процесс говорит всем свое время
          (похоже на запрос на вход в критическую секцию).
       2. При приеме сообщения оно сохраняется вместе со временем и
          отправляется подтверждение, помеченное временем.
       3. Сообщение может быть доставлено процессу, если от всех
          остальных потоков получены сообщения с большей временной
          меткой (этот шаг аналогичен работе критической секции).
     * *Скин*. Используются логические часы Лампорта.

       1. Инициатор отправляет сообщение и время (мультикаст кому
          надо).
       2. При приеме сообщения процесс помечает сообщение как
          недоставленное, отправляет свое время инициатору.
       3. Когда инициатору вернулись все сообщения, он выбирает
          макс. время и снова отправляет сообщение (финальное).
       4. При приеме финального сообщения оно помечается как
          доставленное и доставляется получателю, если оно имеет
          минимальное время в очереди сообщений.
** Системы, ошибки, FLP теорема
   Иерархия ошибок в распределенных системах:
   * Отказ одного или нескольких узлов (/crash/)
   * Отказ одного или нескольких каналов (/link failure/)
   * Ненадежная доставка сообщений (/omission/) -- какие-то сообщения
     могут не доставляться.
   * Византийская ошибка (/byzantine failure/) -- самый сильный класс
     ошибок, когда происходит что-то совершенно непредсказуемое с
     сообщениями (перехват, изменение), любое поведение ноды.

   Системы делятся на синхронные и асинхронные:
   1. *Синхронные* -- время передачи ограничено сверху, можно разбить
      выполнение алгоритма на фазы.
   2. *Асинхронные* -- нету ограничения сверху на передачу сообщений, но
      конечно, если система работает без ошибок.

   Свойства консенсуса в распределенной системе:
   * *Согласие* (/agreement/) -- все (не сбойные) процессы должны
     завершиться с одним и тем же решением.
   * *Нетрививальность* (/non-triviality/) -- должны быть варианты
     исполнения приводящие к разным решениям.
   * *Завершение* (/termination/) -- протокол должен завершаться за
     конечное время.

   Невозможность консенсуса в асинхронной системе с отказом узла (/FLP
   -- Fischer Lynch Paterson/). Возможные предпосылки: асинхронность
   системы, возможность отказа узла, а консенсус нужно достичь за
   конечное время.

   * *Теорема*: для $N$ процессов достижение консенсуса невозможно (даже
     на множестве значений ${0,1}$).

     Доказательство: от противного. Пусть такой алгоритм
     существует. Покажем его несостоятельность.

     *Процесс* -- это автомат, который может:
     * ~receive():msg~ ожидать получения сообщения, причем нет
       таймаутов по условию.
     * ~send(msg)~ отправлять сообщения.
     * ~decided(value)~ -- принимать решение. Можно делать решение
       один раз, но терминирование программы после принятия решения не
       обязательно (можно отправлять решение другим процессам).

     *Конфигурация* -- состояние всех процессов + сообщения в пути.

     *Начальная конфигурация* -- содержит начальные данные для каждого
     из процессов. Не обязательно один бит, а сколько угодно входных
     данных. Каждый процесс может иметь свою программу.

     *Шаг* между конфигурациями:
     * Обработка сообщения процессом (событие)
     * Внутренние действия процесса между ожиданиями
       сообщения. Детерменировано событием.

     *Исполнение* -- бесконечная цепочка шагов от начального состояния
     (процессы не терминируются после принятия решения).

     *Отказавший* процесс делает конечное число шагов в процессе
     исполнения. Такой процесс единственен.

     Любое сообщение для не отказавшего процесса обрабатывается через
     конечное число шагов. Сообщения не теряются.

     Все процессы, пришедшие к решению, имеют одно и то же решение $∈
     {0,1}$. Один процесс имеет право упасть, но остальные должны
     прийти к решению за конечное количество шагов.

     Конфигурация называется:
     * $i$*-валентной*, если все цепочки шагов из нее приводят к
       решению $i ∈ {0,1}$.
     * *бивалентной* -- есть как цепочки шагов приводящие к решению
       $0$, так и к $1$.

     Цепочки шагов с событиями на разных процессах коммутируют и
     приводят к одной и той же конфигурации если поменять их порядок
     выполнения.

     * *Лемма 1*: существует начальная бивалентная конфигурация.

       /Доказательство/:

       От противного. Пусть нету, значит все конфигурации
       одновалентны. Из нетривиальности следует существование и $0$ и
       $1$-валентных конфигураций. Найдем пару начальных конфигураций
       разной валентности, отличающихся начальным состоянием только
       одного процесса. Но этот процесс может отказать с самого
       начала, тогда одна и та же цепочка шагов других процессов будет
       возможна в обеих конфигурациях (упавший процесс не будет
       совершать шагов -- упадет сразу).

     * *Лемма 2*: для бивалентной конфигурации можно найти следующую
       за ней бивалентную.

       Если $G$ бивалентна и $e$ -- какое-то событие (процесс $p$,
       сообщение $m$) в этой конфигурации, то возьмем:
       * $C$ -- множество конфигураций достижимых из $G$ без $e$.
       * $D$ -- множество конфигураций $D = e(C)$, то есть тех, где
         $e$ - последнее событие.

       Покажем, что $D$ содержит бивалентную конфигурацию, тем самым
       придем к противоречию с достижением консенсуса за конечное
       число шагов *и этим докажем теорему*.

       /Доказательство/:

       Воспользуемся асинхронностью: нет предела на время обработки,
       значит любое сообщение можно отложить на любое конечное время.
       Будем доказывать лемму 2 от противного: пусть $D$ не содержит
       бивалентных конфигураций.

       * *Лемма 2.1*: i-валентные конфигурации.

         В $D$ для любого $i$ есть $i$-валентная конфигурация.

         /Доказательство/:

         Так как $G$ бивалентна, то по какой-то цепочке шагов из нее
         можно дойти до $i$-валентной $Eᵢ$.
         * $Eᵢ ∈ D$ -- мы нашли искомую конфигурацию.
         * $Eᵢ ∈ C$ -- $e(C) ∈ D$ искомая конфигурация.
         * Иначе $e$ применялась в цепочке шагов для достижения $Eᵢ ∈
           G$, а значит есть $Fᵢ ∈ D$ сразу после применения $e$, из
           которой доступна $Eᵢ$ по какой-то цепочке шагов. Мы
           предположили, что в $D$ нету бивалентных
           конфигураций. Значит $Fᵢ ∈ D$ $i$-валентна.
       * *Лемма 2.2*: соседние конфигурации.

         Найдем соседние (отличающиеся одним шагом $e'$) $C₀ ∈ C$ и
         $C₁ ∈ C$, что $D₀ = e(C₀) ∈ D$ является $0$-валентной, а $D₁
         = e(C₁) ∈ D$ является $1$-валентной (например). Не теряя
         общности, пусть $C₀ = e'(C₁)$, где событие $e'$ произошло на
         процессе $p'$.

         Как найти: пусть, не теряя общности, $e(G) ∈ D$ $0$-валентна
         (если она $1$-валентна, то симметрично). Она соответствует
         пустой цепочке шагов из $G$.  По лемме 2.1 в $D$ есть
         $1$-валентная конфигурация $D₁ = e(C₁) ∈ D$. Будем убирать из
         цепочки шагов ведущей от $G$ к $C₁$ по одному шагу с конца,
         пока не найдем искомую пару соседей $C₀$ и $C₁$.

         Разбор случаев:
         1. Если $p ≠ p'$, то $e$ и $e'$ коммутируют. Получается, что
            $D₁$ должна быть одновременно $1$- и
            $2$-валентной. Противоречие.
         2. Если $p = p'$, то рассмотрим цепочку шагов $σ$ от
            состояния $C₀$, где процесс $p$ отказал (не выполняется),
            а остальные пришли к решению. Тогда конфигурация $A =
            σ(C₀)$, с решением, должна быть $0$- или $1$-валентной. Но
            $E₀ = e(A) = σ(D₀)$ $0$-валентная, а $E₁ = e(e'(A)) =
            σ(D₁)$ $1$-валентная.

       Итого теорема доказана.

   Результат FLP о невозможности консенсуса верен даже если процессу
   разрешено делать "атомарную передачу" сообщения сразу нескольким
   процессам. Это следует из определения шага от конфигурации к
   конфигурации. Однако, нет гарантии, что все процессы обработают
   сообщение -- один может упасть и не получить.
** Консенсус и TRB
   Если есть гарантия получения сообщения всеми процессами (или ни
   одним), то такая операция называется Terminating Reliable Broadcast
   (/TRB/). С TRB можно тривиально написать алгоритм консенсуса. Но
   суть в том, что сама по себе TRB сложна достаточно.

   Применение консенсуса:
   * Выбор лидера -- каждый процесс предлагает себя, консенсус
     определяет лидера для последующего распределенного алгоритма.
   * TRB -- надо прийти к консенсусу о том, надо ли обрабатывать
     полученное сообщение. Задача TRB эквивалентна задаче
     консенсуса. То есть в асинхронной системе TRB тоже неразрешимa.

   Невозможность консенсуса при наличии ошибок (отказов) доказывается
   только для детерменированных алгоритмов в асинхронной сети. Но
   можно прийти к консенсусу, если:
   * Сделать сеть синхронной (ограничить время доставки сообщений)
   * Сделать алгоритм недетерминированным (случайным)
   * Ослабить требования наличия прогресса в алгоритме

   Алгоритмы консенсуса в синхронных сетях:

   При отказе $f$ узлов: делаем узел с $f+1$ фазой.

   Тупой алгоритм: пусть $V$ -- набор значений, из которого мы
   будем что-то предлагать. Сделам $f+1$ итераций цикла, в которой
   каждый узел отправит всем какое-то новое значение всем другим, а
   также получит какие-то значения. Полученные значения положим в
   $V$. Так вот $f+1$ итераций.

   Согласимся выбрать $min(V)$.


   При византийской ошибке:
   * Решение возможно только при $N > 3f$.
   * 2х-фазный алгоритм решения при $N=4, f=1$. Обобщается на общий
     алгоритм с $f+1$ фазами.
   * Доказательство невозможности при $N=3, f=1$.

** Недетерминированные алгоритмы и Бен-Ор
   Недетерминированные алгоритмы консенсуса:
   * Алгоритм рандомизирован.
   * Достижение консенсуса требуется с вероятностью 1.
   * Порядок исполнения операций в системе выбирает противник (либо
     знающий все, либо как-то ограниченый, слабый).
   * Алгоритм Бен-Ора для бинарного консенсуса работает даже при
     сильном противнике при любом $f < N/2$, но ожидаемое время
     достижения консенсуса $O(2^N)$.

   Алгоритм Бен-Ора (/Ben-Or/): есть множество раундов, по две фазы в
   раунде. На каждой фазе процесс ждет получения $N-f$ сообщений.
   1. Фаза 1. *Предпочтение* рассылает текущее предпочтение посылая
      всем сообщение $(1,k,p)$, где $k$ -- номер раунда. Если больше
      $N/2$ голосов за одно значение, то *ратифицирует* его.
   2. Фаза 2: *Ратификация* шлет $(2,k,v)$, чтобы ратифицировать
      значение или $(2,k,?)$, если не набрал нужное число
      голосов.
      * Получив хотя бы одну ратификацию (свою или от другого) на
        следующий раунд меняет предпочтение на полученное $v$.
      * Получив больше $f$ ратификаций принимает решение $v$,
        продолжает исполняться.
      * Не получив ратификации, меняет предпочтение для следующего
        раунда на случайное.

   Корректность Бен-Ора:
   * *Лемма 1*: в одном раунде разные процессы не могут ратифицировать
     разные значения. Очевидно из-за необходимости набрать больше
     $N/2$ сообщений $(1,k,v)$ для ратификации значения $v$.
   * *Лемма 2*: если в раунде $k$ процесс принял решение $v$, то $k+1$
     раунд все процессы начнут с предпочтением $v$.

     Чтобы принять решение процесс получил минимум $f+1$ сообщений
     вида $(2,k,v)$. Чтобы начать раунд с другим предпочтением процесс
     должен был получить $N-f$ сообщений вида $(2,k,?)$. Сообщений с
     другим $v$ не может быть по лемме 1, а при приеме хотя бы одного
     $(2,k,v)$ он бы взял предпочтение $v$. Но $(f+1) + (N-f) > N$ --
     противоречие.

   Чтобы остановить алгоритм, а не исполнять его бесконечно, надо
   выслать третий тип сообщения "*решение*" как только решение
   принято, и при его получении принимать решение и останавливаться.

   Система асинхронна. Сообщения не обязаны приходить в каждый процесс
   раунд за раундом. Но так как на каждом раунде ждем $N-f$ сообщений,
   то фактически раунды получаются почти синхронными.

   Даже если противник, который решает как приходят сообщения
   (выбирающий цепочку шагов в исполнении), сильный (знает все
   состояние системы), то вероятность завершения алгоритма за конечное
   время все еще единица.
** Replicated State Machine, Paxos
   Имеем некоторое состояние (счет в банке, персонаж в игре), надо
   быстро его обновлять (не храним на диске) и хотим защитить его от
   сбоев конкретного узла.

   Решение -- хранить несколько копий этого состояния на разных узлах
   для надежности. Если операции детерменировано влияют на состояние,
   то можем независимо применять все операции к разным репликам. С
   другой стороны, встает проблема поддержания единого состояния, если
   операции не коммутируют. То есть, при одновременной попытке
   выполнить несколько операций надо прийти к консенсусу, какая будет
   первой.

   Алгормти Paxos. Лампорт 1989. Первый алгоритм асинхронного
   консенсуса.

   Каждый проецсс выбирает одно значение из множества
   предложенных. Гарантирует *согласие* при любых отказах и
   произвольных задержках сообщений. Не гарантирует завершение за
   конечное время (по теореме FLP не может гарантировать. Однако при
   определенных практических реалиях консенсус достигается за конечное
   число шагов).

   Решим задачу однократного консенсуса -- выбора $i$-й операции над
   RSM. Имеем множество *предлагающих* (/proposer/) процессов. Они
   пытаются выполнить операцию над RSM и предлагают свою операцию в
   качестве следующей. Проще всего, когда принимает решение один
   *принимающий* (/acceptor/) процесс. Тогда нету проблемы прийти к
   консенсусу (сообщение пришедшее к принимающему первым и принято),
   но в случае отказа аксептора в системе не будет прогресса, поэтому
   аксепторов должно быть много.

   Нужна возможность узнать о принятом решении некому множеству
   *узнающих* (/learner/) процессов. Принимающие процессы не обязаны
   быть теми же самыми, которые хранят копию состояния RSM, они могут
   только принимать решение о порядке операций, сообщая о нем
   множеству узнающих процессов, которые обновляют у себя состояние
   RSM.

   Таким образом, в общем случае, имеем три роли. На практике они
   могут произвольно совмещаться:
   1. Предлагающие (/proposers/) предлагают значения.
   2. Принимающие (/acceptors/) принимают значения.
   3. Узнающие (/learners/) узнают о решениях.

   Хотим алгоритм, работающий корректно на основе кворума множества
   принимающих значений. Множество принимающих процессоров и
   используемый кворум заранее выбрано и зафиксировано. Кворум любой
   -- большинство, например. Для кворума нужны не все
   процессы. Предполагается, что отказы узлов временные. Отказ --
   уснул и не отвечает.

   Среди принимающих выбираем *лидера*. Каждый предлагающий должен
   знать множество принимающих и кто из них лидер. Лидер меняется,
   если предыдущий отказал. Лидер выполняет вспомогательную функцию в
   алгоритме. Мы не можем гарантироват выбор одного лидера за конечное
   время (FLP и консенсус), но будем это делать все равно. Таким
   образом, лидеров может быть несколько. Разные принимающие будут
   иметь свое мнение о лидере, но алгоритм Paxos будет гарантировать
   *согласие* и в этом случае, но без гарантий завершения пока лидеров
   несколько.

   Для прихода к консенсусу Paxos делает один или несколько раундов
   *голосования*. Раунд инициализируется лидером (предложения
   высылаются ему, а он их ставит на голосование). Несколько раундов
   может случиться если нету согласия о едином лидере или что-то
   отказало и надо голосовать заново. Вся структура алгоритма нужна
   для обеспечения *согласия* несмотря на то, что несколько
   голосований могут проходить одновременно. Каждое запущенное
   голосование имеет уникальный номер $k$ (можно взять пару из
   процесса и локально увеличивающегося счетчика). Лидер может заново
   инициировать голосование если видит что прогресса нету, с другим
   $k$.

   1. 1 фаза голосования:
      1. 1a (*подготовка*): лидер инициирует голосование и посылает
         сообщение $(1a, k)$ кворуму принимающих, где $k$ это
         глобальный уникальный номер голосования.
      2. 1т (*обещание*): получив $(1a, k)$ принимающий обещает не
         принимать предложение с меньшим номером и отвечает $(1b, k,
         ack, k', v')$, где $(k', v')$ это иноформация о *принятом*
         предложении с максимальным номером $k' < k$ (см. фазу 2b),
         где $k' = 0$, если ничего еще не принято. Или сообщает, что
         уже дал другое обещание для $k'' > k$ и отвечает $(1b, k'',
         nack)$, на что лидер повторит $1a$ заново, послав $(1a,
         k'')$.
   2. 2 фаза голосования:
      1. 2a (*запрос*): лидер, получив обещания $(1b, k, ack, k', v')$
         от кворума принимающих, предлагает свое значение. Берет
         значение $v'$ для наибольшего $k'$ полученного от
         аксептеров. А если все $k' == 0$, то предлагает свое значение
         $v$ и посылает запрос $(2a, k, v)$ кворуму принимающих. На
         второй фазе можно использовать другой кворум.
      2. 2b (*подтверждение*): если принимающий получает запрос $(2a,
         k, v)$ и он не давал обещание для $k' > k$, то он *принимает*
         предложение $(k,v)$ и посылает $(2b, k, v)$ всем
         узнающим. Узнающий, получив сообщение $(2b, k, v)$ от кворума
         принимающих, узнает о том, что выбрано $v$.

   Модификации Paxos:
   * Multi paxos -- выполняя paxos много раз подряд, можно делать
     фазу 1 для всех копий сразу (она не зависит от $v$) и,
     подтвердив факт наличия уникального лидера через это, делать для
     каждой копии алгоритма Paxos свою фазу 2. Получаем задержку в 3
     сообщения между proposer-ом и learner-ом.
   * Fast paxos -- посылаем предложения acceptor-ам в обход
     лидера. Задержка в 2 сообщения между proposer-ом и learner-ом
     если нет коллизий.
   * Dynamic paxos -- меняем набор серверов во время работы
   * Cheap paxos -- активно используем только кворум из $f+1$
     сервером и только при отказе подключаем $f$ запасных.
   * Stoppable paxos, byzantine paxos, ...

   Транзакции в распределнных системах. *Транзакция* -- единица работы
   над множеством элементов, хранящихся в БД. *ACID* -- atomicity,
   consistency, isolation, durability. Транзакции деляться на 2PL (2
   phase locking) и 2PC (2 phase commit).
